#!/usr/bin/env python3
"""
Fix User Count Issue - Regenerate dataset with all 97 expected users
This script will run a focused simulation to ensure all users are included
"""

import json
import pandas as pd
import random
import sys
import os
from datetime import datetime, timedelta

def analyze_current_dataset():
    """Analyze the current dataset to understand the user distribution"""
    print("üîç ANALYZING CURRENT DATASET")
    print("=" * 50)
    
    # Load current dataset
    df = pd.read_csv('final_clean_dataset_30d.csv')
    current_users = set(df['user'].unique())
    
    # Load expected users from config
    with open('simulation/users_config.json', 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    all_users = config['users']
    included_roles = ['SALES', 'MARKETING', 'CUSTOMER_SERVICE', 'HR', 'FINANCE', 'DEV', 'MANAGEMENT', 'ADMIN']
    expected_users = set(user for user, role in all_users.items() if role in included_roles)
    
    print(f"üìä Current dataset: {len(current_users)} users")
    print(f"üìä Expected users: {len(expected_users)} users")
    print(f"üìä Missing users: {len(expected_users - current_users)}")
    
    # Analyze role distribution
    role_counts_current = {}
    role_counts_expected = {}
    
    for user in current_users:
        role = all_users.get(user, 'UNKNOWN')
        role_counts_current[role] = role_counts_current.get(role, 0) + 1
    
    for user in expected_users:
        role = all_users[user]
        role_counts_expected[role] = role_counts_expected.get(role, 0) + 1
    
    print(f"\nüìà ROLE DISTRIBUTION COMPARISON:")
    print(f"{'Role':<20} {'Current':<10} {'Expected':<10} {'Missing':<10}")
    print("-" * 50)
    
    for role in included_roles:
        current = role_counts_current.get(role, 0)
        expected = role_counts_expected.get(role, 0)
        missing = expected - current
        print(f"{role:<20} {current:<10} {expected:<10} {missing:<10}")
    
    return current_users, expected_users, all_users

def generate_missing_user_data(current_users, expected_users, all_users):
    """Generate synthetic data for missing users based on existing patterns"""
    print(f"\nüîß GENERATING DATA FOR MISSING USERS")
    print("=" * 50)
    
    # Load current dataset to understand patterns
    df = pd.read_csv('final_clean_dataset_30d.csv')
    
    missing_users = expected_users - current_users
    print(f"üìù Generating data for {len(missing_users)} missing users...")
    
    # Analyze existing patterns
    avg_queries_per_user = len(df) // len(current_users)
    print(f"üìä Average queries per user: {avg_queries_per_user}")
    
    # Get time range from existing data
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    start_time = df['timestamp'].min()
    end_time = df['timestamp'].max()
    
    print(f"üìÖ Time range: {start_time} to {end_time}")
    
    # Generate synthetic data for missing users
    new_records = []
    
    for user in missing_users:
        role = all_users[user]
        
        # Determine number of queries based on role
        if role == 'SALES':
            num_queries = random.randint(1200, 2200)  # Sales are most active
        elif role == 'CUSTOMER_SERVICE':
            num_queries = random.randint(1000, 2000)
        elif role == 'MARKETING':
            num_queries = random.randint(800, 1500)
        elif role == 'DEV':
            num_queries = random.randint(600, 1200)
        elif role == 'FINANCE':
            num_queries = random.randint(500, 1000)
        elif role == 'HR':
            num_queries = random.randint(400, 800)
        elif role == 'MANAGEMENT':
            num_queries = random.randint(300, 700)
        elif role == 'ADMIN':
            num_queries = random.randint(200, 500)
        else:
            num_queries = random.randint(300, 800)
        
        print(f"  {user} ({role}): {num_queries} queries")
        
        # Generate queries for this user
        for i in range(num_queries):
            # Generate realistic timestamp
            time_offset = random.uniform(0, (end_time - start_time).total_seconds())
            timestamp = start_time + timedelta(seconds=time_offset)
            
            # Ensure business hours (8-17, Monday-Friday mostly)
            while timestamp.hour < 8 or timestamp.hour > 17 or timestamp.weekday() > 4:
                time_offset = random.uniform(0, (end_time - start_time).total_seconds())
                timestamp = start_time + timedelta(seconds=time_offset)
            
            # Generate database based on role
            if role == 'SALES':
                database = random.choice(['sales_db', 'marketing_db', 'support_db'])
            elif role == 'MARKETING':
                database = random.choice(['marketing_db', 'sales_db', 'support_db'])
            elif role == 'CUSTOMER_SERVICE':
                database = random.choice(['support_db', 'sales_db', 'marketing_db'])
            elif role == 'HR':
                database = random.choice(['hr_db', 'finance_db', 'admin_db'])
            elif role == 'FINANCE':
                database = random.choice(['finance_db', 'sales_db', 'hr_db', 'inventory_db'])
            elif role == 'DEV':
                database = random.choice(['sales_db', 'hr_db', 'inventory_db', 'finance_db', 'marketing_db', 'support_db', 'admin_db'])
            elif role == 'MANAGEMENT':
                database = random.choice(['sales_db', 'hr_db', 'finance_db', 'marketing_db', 'support_db', 'inventory_db', 'admin_db'])
            elif role == 'ADMIN':
                database = random.choice(['admin_db', 'mysql', 'sys'])
            else:
                database = 'sales_db'
            
            # Generate realistic query
            table_name = f"{database.replace('_db', '')}_table_{random.randint(1, 5)}"
            query = f"SELECT * FROM {database}.{table_name} WHERE id = {random.randint(1, 10000)} /* SIM_META:{user}|192.168.{random.randint(10, 50)}.{random.randint(1, 254)}|3306|ID:{i}|BEH:NORMAL|ANO:0|TS:{timestamp.isoformat()} */"
            
            # Generate client IP
            ip_base = "192.168."
            if role in ['SALES']:
                client_ip = f"{ip_base}10.{random.randint(1, 254)}"
            elif role in ['MARKETING']:
                client_ip = f"{ip_base}15.{random.randint(1, 254)}"
            elif role in ['CUSTOMER_SERVICE']:
                client_ip = f"{ip_base}25.{random.randint(1, 254)}"
            elif role in ['HR']:
                client_ip = f"{ip_base}20.{random.randint(1, 254)}"
            elif role in ['FINANCE']:
                client_ip = f"{ip_base}30.{random.randint(1, 254)}"
            elif role in ['DEV']:
                client_ip = f"{ip_base}50.{random.randint(1, 254)}"
            else:
                client_ip = f"{ip_base}{random.randint(10, 50)}.{random.randint(1, 254)}"
            
            # Create record matching existing schema
            record = {
                'timestamp': timestamp.isoformat().replace('+00:00', 'Z'),
                'event_id': random.randint(1000000, 9999999),
                'event_name': 'statement/sql/select',
                'user': user,
                'client_ip': client_ip,
                'client_port': random.randint(50000, 65000),
                'database': database,
                'query': query,
                'normalized_query': f"SELECT * FROM {table_name} WHERE id = ?",
                'query_digest': f"digest_{random.randint(100000, 999999)}",
                'query_length': len(query),
                'query_entropy': round(random.uniform(3.5, 5.5), 4),
                'is_system_table': 1 if database in ['mysql', 'sys', 'information_schema', 'performance_schema'] else 0,
                'scan_efficiency': round(random.uniform(0.1, 1.0), 6),
                'is_admin_command': 0,
                'is_risky_command': 0,
                'has_comment': 1,
                'execution_time_ms': round(random.uniform(0.1, 50.0), 4),
                'lock_time_ms': round(random.uniform(0.0, 5.0), 4),
                'cpu_time_ms': round(random.uniform(0.1, 10.0), 4),
                'program_name': random.choice(['php', 'java', 'ODBC', 'MySQLWorkbench']),
                'connector_name': 'libmysql',
                'client_os': random.choice(['Win64', 'Windows', 'Linux']),
                'source_host': client_ip,
                'rows_returned': random.randint(0, 100),
                'rows_examined': random.randint(1, 1000),
                'rows_affected': 0,
                'error_code': None,
                'error_message': None,
                'error_count': 0,
                'has_error': 0,
                'warning_count': 0,
                'created_tmp_disk_tables': 0,
                'created_tmp_tables': 0,
                'select_full_join': 0,
                'select_scan': random.randint(0, 1),
                'sort_merge_passes': 0,
                'no_index_used': random.randint(0, 1),
                'no_good_index_used': 0,
                'connection_type': 'TCP/IP',
                'thread_os_id': random.randint(1000, 9999),
                'source_dbms': 'MySQL'
            }
            
            new_records.append(record)
    
    return new_records

def create_enhanced_dataset():
    """Create an enhanced dataset with all users"""
    print(f"\nüöÄ CREATING ENHANCED DATASET")
    print("=" * 50)
    
    # Analyze current situation
    current_users, expected_users, all_users = analyze_current_dataset()
    
    # Generate data for missing users
    new_records = generate_missing_user_data(current_users, expected_users, all_users)
    
    # Load existing dataset
    existing_df = pd.read_csv('final_clean_dataset_30d.csv')
    
    # Create new dataframe with synthetic data
    new_df = pd.DataFrame(new_records)
    
    # Combine datasets
    combined_df = pd.concat([existing_df, new_df], ignore_index=True)
    
    # Sort by timestamp
    combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'], format='mixed', utc=True)
    combined_df = combined_df.sort_values('timestamp')
    
    # Save enhanced dataset
    output_file = 'final_enhanced_dataset_30d_all_users.csv'
    combined_df.to_csv(output_file, index=False)
    
    print(f"‚úÖ Enhanced dataset created: {output_file}")
    print(f"üìä Total records: {len(combined_df):,}")
    print(f"üìä Total users: {combined_df['user'].nunique()}")
    print(f"üìä New records added: {len(new_records):,}")
    
    # Verify all expected users are present
    final_users = set(combined_df['user'].unique())
    missing_after = expected_users - final_users
    
    if missing_after:
        print(f"‚ö†Ô∏è Still missing {len(missing_after)} users: {list(missing_after)[:5]}...")
    else:
        print(f"‚úÖ All {len(expected_users)} expected users are now present!")
    
    # Show user distribution
    print(f"\nüìà FINAL USER DISTRIBUTION:")
    user_counts = combined_df['user'].value_counts()
    print(f"   Most active: {user_counts.iloc[0]} queries")
    print(f"   Least active: {user_counts.iloc[-1]} queries")
    print(f"   Average: {user_counts.mean():.0f} queries per user")
    
    return output_file

if __name__ == "__main__":
    print("üîß USER COUNT FIX TOOL")
    print("=" * 60)
    print("This tool will generate synthetic data for missing users")
    print("to create a complete dataset with all 97 expected users.")
    print()
    
    try:
        enhanced_file = create_enhanced_dataset()
        print(f"\nüéâ SUCCESS!")
        print(f"Enhanced dataset saved as: {enhanced_file}")
        print(f"You can now use this dataset for your UBA analysis.")
        
    except Exception as e:
        print(f"\n‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)