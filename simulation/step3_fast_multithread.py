# simulation/step3_fast_multithread.py
import mysql.connector
from mysql.connector import errorcode
import csv, time, uuid, threading, sys
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- C·∫§U H√åNH ---
SCENARIO_FILE = "simulation/scenario_script_10.csv"
FINAL_DATASET = "final_dataset_10_1.csv"
DB_CONFIG = {
    "user": "root",
    "password": "root", # <- Thay password c·ªßa b·∫°n
    "host": "localhost",
    "database": "mysql"
}

NUM_THREADS = 10      # S·ªë lu·ªìng ch·∫°y song song
BATCH_SIZE = 50       # K√≠ch th∆∞·ªõc m·ªói batch x·ª≠ l√Ω

print_lock = threading.Lock()
total_processed = 0
stop_event = threading.Event()

def get_connection():
    try:
        # Timeout k·∫øt n·ªëi 5s ƒë·ªÉ kh√¥ng ch·ªù l√¢u n·∫øu DB s·∫≠p
        return mysql.connector.connect(**DB_CONFIG, connection_timeout=5, autocommit=True)
    except: return None

def scrub_cursor(cursor):
    """V·ªá sinh cursor ƒë·ªÉ tr√°nh l·ªói Unread result found"""
    try:
        cursor.fetchall()
    except: pass
    try:
        while cursor.nextset():
            try: cursor.fetchall()
            except: pass
    except: pass

def process_batch(batch_data):
    if stop_event.is_set(): return []
    results = []
    conn = get_connection()
    if not conn: return []

    try:
        cursor = conn.cursor(buffered=True)
        
        # --- C·∫§U H√åNH PH√íNG TH·ª¶ CHO TOOL ---
        # Gi·ªõi h·∫°n th·ªùi gian ch·∫°y query l√† 2000ms (2s)
        # ƒê·ªÉ tr√°nh b·ªã treo khi ch·∫°y c√°c query t·∫•n c√¥ng nh∆∞ SLEEP(100) ho·∫∑c DoS
        cursor.execute("SET SESSION MAX_EXECUTION_TIME=2000") 
        
        for row in batch_data:
            if stop_event.is_set(): break
            
            # T·∫°o tag duy nh·∫•t ƒë·ªÉ truy v·∫øt trong performance_schema
            unique_tag = f"/* TAG:{uuid.uuid4().hex[:8]} */"
            tagged_query = f"{row['query']} {unique_tag}"
            
            # M·∫∑c ƒë·ªãnh c√°c gi√° tr·ªã
            rows_sent = 0
            rows_affected = 0
            rows_examined = 0
            lock_time = 0.0
            error_code = 0
            error_msg = ""
            real_exec = 0.0
            
            # 1. CH·∫†Y QUERY
            try:
                cursor.execute(f"USE {row['database']}")
                scrub_cursor(cursor)
                
                cursor.execute(tagged_query)
                
                if cursor.with_rows:
                    res = cursor.fetchall()
                    rows_sent = len(res)
                    rows_affected = 0
                else:
                    rows_sent = 0
                    rows_affected = cursor.rowcount
                    
            except mysql.connector.Error as err:
                error_code = err.errno
                # L√†m s·∫°ch th√¥ng b√°o l·ªói (b·ªè xu·ªëng d√≤ng, nh√°y k√©p ƒë·ªÉ kh√¥ng v·ª° CSV)
                error_msg = str(err.msg).replace('\n', ' ').replace('"', "'")
                if err.errno in [2006, 2013, 2014]: # M·∫•t k·∫øt n·ªëi th√¨ d·ª´ng
                    break 
            except Exception as e:
                error_code = 9999
                error_msg = str(e).replace('\n', ' ')
            finally:
                scrub_cursor(cursor)

            # 2. TR√çCH XU·∫§T FORENSIC DATA T·ª™ PERFORMANCE_SCHEMA
            # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫•t ƒë·ªÉ l·∫•y rows_examined, lock_time
            try:
                metric_sql = f"""
                SELECT 
                    TIMER_WAIT / 1000000000000 as exec_time_sec, -- Chuy·ªÉn Picosecond sang Second
                    LOCK_TIME / 1000000000000 as lock_time_sec,
                    ROWS_EXAMINED,
                    ROWS_SENT,
                    ROWS_AFFECTED,
                    CREATED_TMP_DISK_TABLES
                FROM performance_schema.events_statements_history_long
                WHERE SQL_TEXT LIKE '%{unique_tag}%'
                ORDER BY EVENT_ID DESC LIMIT 1
                """
                cursor.execute(metric_sql)
                metric = cursor.fetchone()
                
                if metric:
                    real_exec = float(metric[0]) if metric[0] else 0.0
                    lock_time = float(metric[1]) if metric[1] else 0.0
                    rows_examined = int(metric[2]) if metric[2] else 0
                    # ∆Øu ti√™n l·∫•y rows_sent t·ª´ performance_schema n·∫øu c√≥
                    if metric[3] is not None: rows_sent = int(metric[3])
                    
                    # N·∫øu query b·ªã l·ªói Timeout, performance schema v·∫´n ghi l·∫°i time
                    if error_code == 3024: # Query execution was interrupted
                         error_msg = "Query execution time exceeded limit (Simulated DoS prevention)"
                
            except: 
                pass # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c metric th√¨ ch·∫•p nh·∫≠n d√πng gi√° tr·ªã m·∫∑c ƒë·ªãnh
            finally: 
                scrub_cursor(cursor)

            # 3. GHI LOG V√ÄO LIST
            results.append({
                "timestamp": row['timestamp'],
                "user": row['user'],
                # Gi·∫£ l·∫≠p IP d·ª±a tr√™n user ƒë·ªÉ IP c·ªë ƒë·ªãnh cho t·ª´ng user (t·ªët cho ML h·ªçc pattern)
                "client_ip": f"192.168.1.{hash(row['user']) % 250 + 1}",
                "database": row['database'],
                "query": row['query'],
                # C√°c tr∆∞·ªùng Metrics quan tr·ªçng
                "execution_time_sec": real_exec,
                "rows_returned": rows_sent,
                "rows_examined": rows_examined,  # <--- M·ªõi th√™m
                "rows_affected": rows_affected,
                "lock_time_sec": lock_time,      # <--- M·ªõi th√™m
                # Th√¥ng tin l·ªói
                "error_code": error_code,
                "error_message": error_msg,
                # Nh√£n (Label)
                "is_anomaly": row['is_anomaly'],
                "behavior_type": row.get('behavior_type', 'NORMAL'),
                "source_dbms": "MySQL"
            })
            
    except Exception: pass
    finally:
        try:
            if 'cursor' in locals(): cursor.close()
            if conn.is_connected(): conn.close()
        except: pass
        
    return results

def run_simulation():
    global total_processed
    print(f"üìñ ƒêang ƒë·ªçc k·ªãch b·∫£n: {SCENARIO_FILE}...")
    try:
        with open(SCENARIO_FILE, 'r', encoding='utf-8') as f:
            scenarios = list(csv.DictReader(f))
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file k·ªãch b·∫£n: {e}")
        return

    total_rows = len(scenarios)
    print(f"üöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y SIMULATION (Forensics Mode Enabled)")
    print(f"   - T·ªïng s·ªë d√≤ng: {total_rows}")
    print(f"   - Threads: {NUM_THREADS}")
    print(f"   - Timeout b·∫£o v·ªá: 2 gi√¢y/query")
    
    batches = [scenarios[i:i + BATCH_SIZE] for i in range(0, total_rows, BATCH_SIZE)]
    final_data = []
    start_time = time.time()
    
    executor = ThreadPoolExecutor(max_workers=NUM_THREADS)
    try:
        futures = [executor.submit(process_batch, batch) for batch in batches]
        for future in as_completed(futures):
            if stop_event.is_set(): break
            try:
                res = future.result()
                if res:
                    final_data.extend(res)
                    with print_lock:
                        total_processed += len(res)
                        if total_processed % 200 == 0 or total_processed == total_rows:
                            elapsed = time.time() - start_time
                            speed = total_processed / elapsed if elapsed > 0 else 0
                            # In ƒë√® d√≤ng c≈© cho ƒë·∫πp
                            print(f"\r‚ö° Progress: {total_processed}/{total_rows} | Speed: {speed:.1f} q/s | Errors Detected: {len([x for x in final_data if x['error_code'] != 0])}", end="")
            except: pass
    except KeyboardInterrupt:
        print("\nüõë ƒêang d·ª´ng kh·∫©n c·∫•p...")
        stop_event.set()
        executor.shutdown(wait=False)
        
    if final_data:
        print(f"\n\nüíæ ƒêang l∆∞u file '{FINAL_DATASET}'...")
        df = pd.DataFrame(final_data)
        # S·∫Øp x·∫øp l·∫°i theo th·ªùi gian
        df.sort_values(by='timestamp', inplace=True)
        
        # L∆∞u file
        df.to_csv(FINAL_DATASET, index=False)
        print(f"‚úÖ HO√ÄN T·∫§T! File k·∫øt qu·∫£: {FINAL_DATASET}")
        print(f"   -> S·ªë c·ªôt: {len(df.columns)}")
        print(f"   -> Bao g·ªìm: execution_time_sec, lock_time_sec, rows_examined...")
    else:
        print("\n‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω.")

if __name__ == "__main__":
    # B·∫≠t ch·∫ø ƒë·ªô theo d√µi c·ªßa MySQL tr∆∞·ªõc khi ch·∫°y
    try:
        print("üîß ƒêang c·∫•u h√¨nh MySQL Performance Schema...")
        c = mysql.connector.connect(**DB_CONFIG)
        cur = c.cursor()
        # B·∫≠t consumer l·ªãch s·ª≠ c√¢u l·ªánh
        cur.execute("UPDATE performance_schema.setup_consumers SET ENABLED='YES' WHERE NAME LIKE 'events_statements_history_long'")
        # B·∫≠t instrument ƒëo th·ªùi gian
        cur.execute("UPDATE performance_schema.setup_instruments SET ENABLED='YES', TIMED='YES' WHERE NAME LIKE 'statement/%'")
        c.commit()
        c.close()
    except Exception as e:
        print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng th·ªÉ c·∫•u h√¨nh Performance Schema. S·ªë li·ªáu th·ªùi gian c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c. L·ªói: {e}")
        
    run_simulation()