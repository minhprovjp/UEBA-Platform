"""
================================================================================
MODULE X·ª¨ L√ù D·ªÆ LI·ªÜU CH√çNH
================================================================================
T√≠ch h·ª£p:
- Feature engineering c·∫•p nghi√™n c·ª©u (sqlglot + behavioral z-score + entropy)
- Semi-supervised ML: Isolation Forest ‚Üí pseudo-label ‚Üí LightGBM (self-training)
- Persistent Buffering: L∆∞u cache training xu·ªëng ƒëƒ©a ƒë·ªÉ kh√¥ng m·∫•t d·ªØ li·ªáu khi restart
"""

import os
import sys
import logging
import joblib
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.ensemble import IsolationForest
from datetime import datetime
import hashlib
from pathlib import Path
import time

# --- Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [DataProcessor-PROD] - %(message)s')
logger = logging.getLogger(__name__)

# --- Paths ---
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import MODELS_DIR, ACTIVE_RESPONSE_TRIGGER_THRESHOLD, SENSITIVE_TABLES_DEFAULT, ALLOWED_USERS_FOR_SENSITIVE_DEFAULT
from engine.features import enhance_features_batch
from engine.utils import (
    is_late_night_query, is_potential_large_dump,
    analyze_sensitive_access, check_unusual_user_activity_time,
    is_suspicious_function_used, is_privilege_change, get_normalized_query
)


# Production model paths
PROD_MODEL_PATH = os.path.join(MODELS_DIR, "lgb_uba_production.joblib")
FALLBACK_MODEL_PATH = os.path.join(MODELS_DIR, "lgb_uba_fallback.joblib")
BUFFER_FILE_PATH = os.path.join(MODELS_DIR, "training_buffer_cache.parquet")

# File ƒë·ªÉ l∆∞u danh s√°ch category l√∫c train
CAT_MAP_PATH = os.path.join(MODELS_DIR, "cat_features_map.joblib") 

os.makedirs(MODELS_DIR, exist_ok=True)


class ProductionUBAEngine:
    def __init__(self):
        self.model = None
        self.fallback_model = None
        self.features = None
        self.model_version = "unknown"
        self.last_trained = None
        self.cat_mapping = {}
        
        # C·∫•u h√¨nh Training
        self.MIN_TRAIN_SIZE = 1      # Train ngay khi c√≥ 1 d√≤ng
        self.MAX_BUFFER_SIZE = 5000   # Gi·ªØ t·ªëi ƒëa 5000 d√≤ng ƒë·ªÉ train (Sliding Window)
        
        # Bi·∫øn ƒë·ªÉ ki·ªÉm so√°t t·∫ßn su·∫•t ghi ƒëƒ©a
        self.last_save_time = time.time()
        self.SAVE_INTERVAL_SEC = 60 # Ch·ªâ ghi xu·ªëng ƒëƒ©a m·ªói 60s ho·∫∑c khi buffer ƒë·∫ßy
        
        # Kh·ªüi t·∫°o Buffer t·ª´ ƒëƒ©a (Persistence)
        self.training_buffer = self._load_buffer_from_disk()
        self.load_models()

    def _load_buffer_from_disk(self) -> pd.DataFrame:
        """Load d·ªØ li·ªáu c≈© t·ª´ ƒëƒ©a l√™n RAM khi kh·ªüi ƒë·ªông l·∫°i"""
        if os.path.exists(BUFFER_FILE_PATH):
            try:
                df = pd.read_parquet(BUFFER_FILE_PATH)
                logger.info(f"üîÑ Restored training buffer from disk: {len(df)} rows.")
                return df
            except Exception as e:
                logger.error(f"Failed to load buffer file: {e}")
        return pd.DataFrame()

    def _save_buffer_to_disk(self, force=False):
        """L∆∞u Buffer xu·ªëng ƒëƒ©a v·ªõi c∆° ch·∫ø Rate Limit"""
        now = time.time()
        # Ch·ªâ ghi n·∫øu force=True HO·∫∂C ƒë√£ qu√° 60s HO·∫∂C buffer > 1000 d√≤ng m·ªõi
        if not force and (now - self.last_save_time < self.SAVE_INTERVAL_SEC):
            return

        try:
            # √âp ki·ªÉu c√°c c·ªôt string d·ªÖ g√¢y l·ªói tr∆∞·ªõc khi l∆∞u
            str_cols = ['error_message', 'query', 'normalized_query', 'query_digest', 
                        'user', 'database', 'client_ip', 'connection_type', 'command_type', 
                        'event_name', 'suspicious_func_name', 'privilege_cmd_name', 
                        'unusual_activity_reason']
            
            df_to_save = self.training_buffer.copy()
            for col in str_cols:
                if col in df_to_save.columns:
                    df_to_save[col] = df_to_save[col].astype(str)
            
            # Parquet ghi r·∫•t nhanh
            df_to_save.to_parquet(BUFFER_FILE_PATH, index=False)
            self.last_save_time = now
            # logging.info("Persistent buffer saved to disk.")
        except Exception as e:
            logger.error(f"Failed to persist buffer to disk: {e}")
            
    def load_models(self):
        """Load production ‚Üí fallback ‚Üí train new"""
        if os.path.exists(PROD_MODEL_PATH):
            try:
                data = joblib.load(PROD_MODEL_PATH)
                self.model = data['model']
                self.features = data['features']
                if os.path.exists(CAT_MAP_PATH):
                    self.cat_mapping = joblib.load(CAT_MAP_PATH)
                self.model_version = data.get('version', 'v0')
                self.last_trained = data.get('trained_at', 'unknown')
                logger.info(f"Loaded PRODUCTION model v{self.model_version} (trained: {self.last_trained})")
                return
            except Exception as e:
                logger.error(f"Failed to load production model: {e}")

        if os.path.exists(FALLBACK_MODEL_PATH):
            try:
                data = joblib.load(FALLBACK_MODEL_PATH)
                self.model = data['model']
                self.features = data['features']
                logger.warning("Loaded FALLBACK model")
                return
            except Exception as e:
                logger.error(f"Failed to load fallback: {e}")

        logger.warning("No model found.")
        # N·∫øu kh√¥ng c√≥ model nh∆∞ng c√≥ buffer c≈© -> Train ngay l·∫≠p t·ª©c
        if not self.training_buffer.empty and len(self.training_buffer) >= self.MIN_TRAIN_SIZE:
            logger.info("No model found but buffer exists. Training immediately...")
            self._train_core()

    def save_production_model(self, model, features):
        """Atomic save with versioning"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        version_hash = hashlib.md5(str(features).encode()).hexdigest()[:8]
        
        # T·ª± ƒë·ªông tƒÉng version d·ª±a tr√™n s·ªë file hi·ªán c√≥
        try:
            existing = [f for f in os.listdir(MODELS_DIR) if f.startswith('lgb_uba_')]
            ver_num = len(existing)
        except: ver_num = 0
            
        version = f"v{ver_num}_{version_hash}"

        data = {
            'model': model,
            'features': features,
            'version': version,
            'trained_at': datetime.now().isoformat(),
            'feature_count': len(features)
        }

        # Save new production
        joblib.dump(data, PROD_MODEL_PATH)
        # Lu√¥n c·∫≠p nh·∫≠t fallback ƒë·ªÉ an to√†n
        joblib.dump(data, FALLBACK_MODEL_PATH)
        logger.info(f"‚úÖ New PRODUCTION model saved: {version}")

    def _train_core(self):
        """H√†m train n·ªôi b·ªô - T√°ch ra ƒë·ªÉ t√°i s·ª≠ d·ª•ng"""
        if len(self.training_buffer) < self.MIN_TRAIN_SIZE:
            return False
        
        # Danh s√°ch c√°c c·ªôt KH√îNG d√πng cho Machine Learning
        exclude_cols = [
            # 1. ƒê·ªãnh danh & Th·ªùi gian (Metadata)
            'timestamp', 
            'event_id', 
            'thread_os_id',
            'source_dbms',      # H·∫±ng s·ªë (lu√¥n l√† MySQL)
            'client_port',      # Port client thay ƒë·ªïi ng·∫´u nhi√™n (Ephemeral port)
            
            # 2. VƒÉn b·∫£n th√¥ (Raw Text) - Model kh√¥ng hi·ªÉu ƒë∆∞·ª£c
            'query', 
            'normalized_query', 
            'error_message',    # N·ªôi dung l·ªói bi·∫øn thi√™n qu√° nhi·ªÅu
            'query_digest',     # Hash chu·ªói (Cardinallity qu√° cao, d·ªÖ g√¢y overfit n·∫øu data √≠t)
            
            # 3. K·∫øt qu·∫£ ƒë·∫ßu ra (Label Leakage) - C·∫•m k·ªµ ƒë∆∞a v√†o input
            'is_anomaly', 
            'ml_anomaly_score', 
            'unusual_activity_reason',
            'analysis_type',
            
            # 4. C√°c c·ªôt ph·ª• tr·ª£ / JSON
            'accessed_tables', 
            'sensitive_access_info', 
            'tables_touched',
            'suspicious_func_name', 
            'privilege_cmd_name',
            
            # 5. M√£ l·ªói c·ª• th·ªÉ (Optional)
            # N√™n b·ªè error_code v√¨ n√≥ l√† d·∫°ng Category c√≥ qu√° nhi·ªÅu gi√° tr·ªã (null, 1064, 1146...)
            # Ta ƒë√£ c√≥ 'has_error' v√† 'error_count' ƒë·∫°i di·ªán t·ªët h∆°n.
            'error_code' 
        ]

        # N·∫øu ch∆∞a c√≥ feature list, t·ª± ƒë·ªông ch·ªçn
        if not self.features:
            # L·∫•y t·∫•t c·∫£ c·ªôt s·ªë v√† category
            potential_feats = self.training_buffer.select_dtypes(include=[np.number, 'category', 'object']).columns.tolist()
            
            # L·ªçc b·ªè c√°c c·ªôt trong blacklist
            self.features = [f for f in potential_feats if f not in exclude_cols]
            
            # Log ra ƒë·ªÉ ki·ªÉm tra xem Model ƒëang d√πng feature g√¨
            logger.info(f"üöÄ Model Features ({len(self.features)}): {self.features}")

        # T·∫°o X cho LightGBM (gi·ªØ nguy√™n category)
        X = self.training_buffer[self.features].copy()
        
        # Chu·∫©n h√≥a Category khi Train
        cat_cols = ['user', 'client_ip', 'database', 'command_type']
        current_mapping = {}
        
        # X·ª≠ l√Ω NaN v√† ki·ªÉu d·ªØ li·ªáu cho X
        for col in X.columns:
            if col in cat_cols or X[col].dtype == 'object':
                X[col] = X[col].astype(str).astype('category')
                # L∆∞u l·∫°i danh s√°ch category ƒë√£ bi·∫øt
                current_mapping[col] = X[col].cat.categories.tolist()
            else:
                X[col] = X[col].fillna(0)
        
        # L∆∞u mapping
        self.cat_mapping = current_mapping
        joblib.dump(self.cat_mapping, CAT_MAP_PATH)

        try:
            # T·∫°o b·∫£n sao X_iso ƒë∆∞·ª£c m√£ h√≥a s·ªë h·ªçc cho Isolation Forest
            X_iso = X.copy()
            for col in X_iso.columns:
                if X_iso[col].dtype.name == 'category':
                    # Chuy·ªÉn chu·ªói th√†nh s·ªë (0, 1, 2...)
                    X_iso[col] = X_iso[col].cat.codes

            # 1. Pipeline: Isolation Forest (D√πng X_iso to√†n s·ªë)
            iso = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)
            iso.fit(X_iso)
            high_conf_anoms_mask = (iso.predict(X_iso) == -1)
            
            # 2. T·∫°o t·∫≠p train cho Supervised Model (LightGBM)
            # D√πng X g·ªëc (c√≥ category) v√¨ LightGBM x·ª≠ l√Ω t·ªët h∆°n
            X_train = pd.concat([X, X[high_conf_anoms_mask]])
            y_train = np.concatenate([np.zeros(len(X)), np.ones(sum(high_conf_anoms_mask))])

            # 3. Train LightGBM
            new_model = lgb.LGBMClassifier(
                n_estimators=100,
                learning_rate=0.05,
                max_depth=5,
                num_leaves=31,
                scale_pos_weight=10,
                random_state=42,
                n_jobs=-1,
                verbose=-1
            )

            cat_cols = [c for c in ['user', 'client_ip', 'database', 'command_type'] if c in X.columns]
            new_model.fit(X_train, y_train, categorical_feature=cat_cols)

            self.save_production_model(new_model, self.features)
            self.model = new_model
            return True
            
        except Exception as e:
            logger.error(f"Training core failed: {e}", exc_info=True)
            return False

    def train_and_update(self, df_enhanced):
        """
        Public method g·ªçi khi c√≥ d·ªØ li·ªáu m·ªõi: T√≠ch l≈©y -> L∆∞u -> Train
        """
        if df_enhanced.empty:
            return False

        # 1. C·ªông d·ªìn v√†o RAM
        self.training_buffer = pd.concat([self.training_buffer, df_enhanced], ignore_index=True)
        
        # 2. C·∫Øt b·ªõt n·∫øu qu√° l·ªõn (Sliding Window)
        if len(self.training_buffer) > self.MAX_BUFFER_SIZE:
            self.training_buffer = self.training_buffer.iloc[-self.MAX_BUFFER_SIZE:]

        # 3. L∆∞u ngay xu·ªëng ƒëƒ©a (Persistence)
        self._save_buffer_to_disk(force=False)

        # 4. Train ngay l·∫≠p t·ª©c n·∫øu ƒë·ªß d·ªØ li·ªáu
        logger.info(f"Training triggered. Buffer size: {len(self.training_buffer)}")
        return self._train_core()


# Global engine instance
uba_engine = ProductionUBAEngine()


def load_and_process_data(input_df: pd.DataFrame, config_params: dict) -> dict:
    global uba_engine

    if input_df is None or input_df.empty:
        return {"all_logs": pd.DataFrame(), "anomalies_ml": pd.DataFrame()}

    df_logs = input_df.copy()

    # Basic cleanup
    for col in ['rows_returned', 'rows_affected', 'execution_time_ms']:
        if col not in df_logs.columns:
            df_logs[col] = 0
        df_logs[col] = df_logs[col].fillna(0)

    df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'], errors='coerce', utc=True)
    df_logs = df_logs.dropna(subset=['timestamp']).reset_index(drop=True)
    df_logs['query'] = df_logs['query'].astype(str)
    
    df_logs['normalized_query'] = df_logs['query'].apply(get_normalized_query)
    df_logs['query_length'] = df_logs['normalized_query'].str.len()

    if df_logs.empty:
        return {"all_logs": df_logs}

    # ADVANCED FEATURES (Features.py enhancement)
    df_enhanced, ML_FEATURES = enhance_features_batch(df_logs.copy())
    df_logs = df_enhanced

    # [UPDATE] Always update buffer and attempt training
    # Kh√¥ng c·∫ßn ch·ªù 10000 d√≤ng n·ªØa, buffer s·∫Ω t·ª± lo
    try:
        uba_engine.train_and_update(df_logs)
    except Exception as e:
        logger.error(f"Auto-retrain trigger failed: {e}")

    # ML Scoring
    if uba_engine.model and uba_engine.features:
        try:
            # 1. Chu·∫©n b·ªã X v·ªõi ƒë√∫ng c√°c c·ªôt features m√¥ h√¨nh c·∫ßn
            X = df_logs.copy()
            for f in uba_engine.features:
                if f not in X.columns:
                    X[f] = 0
            X = X[uba_engine.features]

            # 2. [FIX] √âp ki·ªÉu Category t∆∞·ªùng minh cho gi·ªëng l√∫c Train
            cat_cols = ['user', 'client_ip', 'database', 'command_type']
            for col in X.columns:
                if col in uba_engine.cat_mapping:
                    # √âp ki·ªÉu v·ªÅ Category v·ªõi ƒë√∫ng danh s√°ch ƒë√£ h·ªçc
                    known_cats = uba_engine.cat_mapping[col]
                    X[col] = X[col].astype(str).astype(pd.CategoricalDtype(categories=known_cats))
                    # C√°c gi√° tr·ªã l·∫° s·∫Ω t·ª± ƒë·ªông th√†nh NaN -> Fill 'unknown' n·∫øu 'unknown' c√≥ trong list, ko th√¨ fill mode
                    if 'unknown' in known_cats:
                        X[col] = X[col].fillna('unknown')
                    else:
                        # Fallback v·ªÅ category ƒë·∫ßu ti√™n n·∫øu kh√¥ng c√≥ unknown
                        X[col] = X[col].fillna(known_cats[0])
                else:
                    # C√°c c·ªôt s·ªë th√¨ √©p v·ªÅ float/int v√† ƒëi·ªÅn 0
                    X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)

            # 3. D·ª± ƒëo√°n
            scores = uba_engine.model.predict_proba(X)[:, 1]
            df_logs['ml_anomaly_score'] = scores
            
            # Ng∆∞·ª°ng ƒë·ªông: L·∫•y top 1% ho·∫∑c > 0.75
            threshold = max(np.quantile(scores, 0.99), 0.75)
            anomalies_ml = df_logs[scores >= threshold].copy()
            
        except Exception as e:
            logger.error(f"ML inference failed: {e}. Using rule-only mode.")
            df_logs['ml_anomaly_score'] = 0.0
            anomalies_ml = pd.DataFrame()
    else:
        df_logs['ml_anomaly_score'] = 0.0
        anomalies_ml = pd.DataFrame()

    # === 4. RULE-BASED DETECTION (gi·ªØ nguy√™n + m·∫°nh h∆°n) ===
    logging.info("Running rule-based detection...")
    rules_config = config_params or {}

    # L·∫•y c·∫•u h√¨nh rule
    from datetime import time as dt_time
    p_late_night_start_time = dt_time.fromisoformat(rules_config.get('p_late_night_start_time', '00:00:00'))
    p_late_night_end_time = dt_time.fromisoformat(rules_config.get('p_late_night_end_time', '05:00:00'))
    p_known_large_tables = rules_config.get('p_known_large_tables', [])
    p_time_window_minutes = int(rules_config.get('p_time_window_minutes', 5))
    p_min_distinct_tables = int(rules_config.get('p_min_distinct_tables', 3))
    p_sensitive_tables = rules_config.get('p_sensitive_tables', []) or SENSITIVE_TABLES_DEFAULT
    p_allowed_users_sensitive = rules_config.get('p_allowed_users_sensitive', []) or ALLOWED_USERS_FOR_SENSITIVE_DEFAULT
    p_safe_hours_start = int(rules_config.get('p_safe_hours_start', 8))
    p_safe_hours_end = int(rules_config.get('p_safe_hours_end', 18))
    p_safe_weekdays = rules_config.get('p_safe_weekdays', [0, 1, 2, 3, 4])
    p_min_queries_for_profile = int(rules_config.get('p_min_queries_for_profile', 10))
    p_quantile_start = float(rules_config.get('p_quantile_start', 0.15))
    p_quantile_end = float(rules_config.get('p_quantile_end', 0.85))

    # Rule 1: Late night
    df_logs['is_late_night'] = df_logs['timestamp'].apply(
        lambda ts: is_late_night_query(ts, p_late_night_start_time, p_late_night_end_time)
    )
    anomalies_late_night = df_logs[df_logs['is_late_night']].copy()

    # Rule 2: Large dump
    df_logs['is_potential_dump'] = df_logs.apply(
        lambda row: is_potential_large_dump(row, p_known_large_tables), axis=1
    )
    anomalies_large_dump = df_logs[df_logs['is_potential_dump']].copy()

    # ---------------------------------------------------------
    # Rule 3: Multi-table in short window (ƒê√É S·ª¨A L·ªñI ƒê·∫æM K√ù T·ª∞)
    # ---------------------------------------------------------
    import ast

    # H√†m bi·∫øn chu·ªói "['a', 'b']" th√†nh list ['a', 'b']
    def parse_list_safe(x):
        if isinstance(x, list): return x
        if isinstance(x, str):
            try: return ast.literal_eval(x)
            except: return []
        return []

    # Parse c·ªôt accessed_tables TR∆Ø·ªöC khi x·ª≠ l√Ω
    # L∆∞u √Ω: ƒê·∫£m b·∫£o c·ªôt accessed_tables t·ªìn t·∫°i trong df_logs
    df_logs['accessed_tables_parsed'] = df_logs['accessed_tables'].apply(parse_list_safe)

    anomalies_multiple_tables_list = []
    window = pd.Timedelta(minutes=p_time_window_minutes)

    for user, group in df_logs.groupby('user', observed=False):
        if len(group) < 2: continue
        group = group.sort_values('timestamp').reset_index(drop=True)
        
        session_tables = set()
        session_queries = []
        start_time = group.iloc[0]['timestamp']

        for _, row in group.iterrows():
            # QUAN TR·ªåNG: D√πng c·ªôt ƒë√£ parse
            current_tables = row['accessed_tables_parsed']
            if not current_tables: continue
            
            tables = set(current_tables)

            if (row['timestamp'] - start_time) > window:
                # Ki·ªÉm tra session c≈©
                if len(session_tables) >= p_min_distinct_tables:
                    anomalies_multiple_tables_list.append({
                        'user': user,
                        'start_time': start_time,
                        'end_time': session_queries[-1]['timestamp'],
                        'distinct_tables_count': len(session_tables),
                        'tables_accessed_in_session': sorted(list(session_tables)),
                        'queries_details': session_queries,
                        # C√°c tr∆∞·ªùng b·∫Øt bu·ªôc cho AggregateAnomaly
                        'anomaly_type': 'multi_table_access',
                        'severity': 0.8,
                        'reason': f"Accessed {len(session_tables)} distinct tables in short window",
                        'scope': 'session',
                        'database': row.get('database', 'unknown'),
                        'details': {"tables": sorted(list(session_tables))}
                    })
                # Reset
                session_tables = tables
                session_queries = [{'timestamp': row['timestamp'], 'query': row['query']}]
                start_time = row['timestamp']
            else:
                session_tables.update(tables)
                session_queries.append({'timestamp': row['timestamp'], 'query': row['query']})
        
        # Ki·ªÉm tra session cu·ªëi c√πng
        if len(session_tables) >= p_min_distinct_tables:
            anomalies_multiple_tables_list.append({
                'user': user,
                'start_time': start_time,
                'end_time': session_queries[-1]['timestamp'],
                'distinct_tables_count': len(session_tables),
                'tables_accessed_in_session': sorted(list(session_tables)),
                'queries_details': session_queries,
                'anomaly_type': 'multi_table_access',
                'severity': 0.8,
                'reason': f"Accessed {len(session_tables)} distinct tables in short window",
                'scope': 'session',
                'database': session_queries[0].get('database', 'unknown') if session_queries else 'unknown',
                'details': {"tables": sorted(list(session_tables))}
            })
            
    anomalies_multiple_tables_df = pd.DataFrame(anomalies_multiple_tables_list)

    # Rule 4: Sensitive access
    df_logs['sensitive_access_info'] = df_logs.apply(
        lambda row: analyze_sensitive_access(row, p_sensitive_tables, p_allowed_users_sensitive,
                                             p_safe_hours_start, p_safe_hours_end, p_safe_weekdays),
        axis=1
    )
    anomalies_sensitive_access = df_logs[df_logs['sensitive_access_info'].notna()].copy()
    if not anomalies_sensitive_access.empty:
        expanded = anomalies_sensitive_access['sensitive_access_info'].apply(pd.Series)
        anomalies_sensitive_access = pd.concat([anomalies_sensitive_access.drop(columns=['sensitive_access_info'], errors='ignore'), expanded], axis=1)

    # Rule 5: Unusual user time
    user_profiles = {}
    for user, g in df_logs.groupby('user', observed=False):
        if len(g) >= p_min_queries_for_profile:
            hours = g['timestamp'].dt.hour
            user_profiles[user] = {
                'active_start': int(hours.quantile(p_quantile_start)),
                'active_end': int(hours.quantile(p_quantile_end))
            }
    df_logs['unusual_activity_reason'] = df_logs.apply(
        lambda row: check_unusual_user_activity_time(row, user_profiles), axis=1
    )
    anomalies_unusual_user_time = df_logs[df_logs['unusual_activity_reason'].notna()].copy()

    # Rule 6 & 7: SQLi + Privilege
    sqli_res = df_logs['query'].apply(is_suspicious_function_used).apply(pd.Series)
    sqli_res.columns = ['is_suspicious_func', 'suspicious_func_name']
    df_logs = pd.concat([df_logs, sqli_res], axis=1)
    anomalies_sqli = df_logs[df_logs['is_suspicious_func'] == True].copy()

    priv_res = df_logs['query'].apply(is_privilege_change).apply(pd.Series)
    priv_res.columns = ['is_privilege_change', 'privilege_cmd_name']
    df_logs = pd.concat([df_logs, priv_res], axis=1)
    anomalies_privilege = df_logs[df_logs['is_privilege_change'] == True].copy()


    # # ========================================================
    # # CHU·∫®N B·ªä D·ªÆ LI·ªÜU ACTIVE RESPONSE
    # # ========================================================
    # users_to_lock_list = []
    # # 1. Thu th·∫≠p t·∫•t c·∫£ c√°c vi ph·∫°m
    # list_of_violation_dfs = []

    # # C√°c vi ph·∫°m d·∫°ng "point-in-time"
    # point_in_time_anomalies = [
    #     anomalies_late_night, anomalies_large_dump, anomalies_sensitive_access,
    #     anomalies_unusual_user_time
    # ]
    # for df in point_in_time_anomalies:
    #     if not df.empty and 'user' in df.columns and 'client_ip' in df.columns:
    #         list_of_violation_dfs.append(df[['user', 'client_ip']])

    # # C√°c vi ph·∫°m d·∫°ng "session" (multi_table)
    # if not anomalies_multiple_tables_df.empty:
    #     session_violations = []
    #     for _, row in anomalies_multiple_tables_df.iterrows():
    #         user = row['user']
    #         if row['queries_details']:
    #             client_ip = row['queries_details'][0].get('client_ip')
    #             if user and client_ip:
    #                 session_violations.append({'user': user, 'client_ip': client_ip})

    #     if session_violations:
    #         list_of_violation_dfs.append(pd.DataFrame(session_violations))

    # # 2. T·ªïng h·ª£p, ƒë·∫øm v√† l·ªçc c√°c user v∆∞·ª£t ng∆∞·ª°ng
    # if list_of_violation_dfs:
    #     all_violations_df = pd.concat(list_of_violation_dfs, ignore_index=True)

    #     # T·ªïng h·ª£p vi ph·∫°m THEO USER
    #     user_violation_counts = all_violations_df.groupby('user').size().reset_index(name='total_violation_count')

    #     # L·ªçc ra c√°c user v∆∞·ª£t ng∆∞·ª°ng T·ªîNG
    #     offenders = user_violation_counts[
    #         user_violation_counts['total_violation_count'] >= ACTIVE_RESPONSE_TRIGGER_THRESHOLD
    #         ]
    #     # Chuy·ªÉn th√†nh list dictionary ƒë·ªÉ truy·ªÅn ƒëi
    #     if not offenders.empty:
    #         users_to_lock_list = offenders.to_dict('records')
    # # =============================================================

    # Normal activities
    anomalous_indices = set(anomalies_ml.index)
    for df_anom in [anomalies_late_night, anomalies_large_dump, anomalies_sensitive_access,
                    anomalies_unusual_user_time, anomalies_sqli, anomalies_privilege]:
        if not df_anom.empty:
            anomalous_indices.update(df_anom.index)
    normal_activities = df_logs[~df_logs.index.isin(anomalous_indices)].copy()

    # === K·∫æT QU·∫¢ ===
    results = {
        "all_logs": df_logs,
        "anomalies_ml": anomalies_ml,
        "anomalies_late_night": anomalies_late_night,
        "anomalies_dump": anomalies_large_dump,
        "anomalies_multi_table": anomalies_multiple_tables_df,
        "anomalies_sensitive": anomalies_sensitive_access,
        "anomalies_user_time": anomalies_unusual_user_time,
        "anomalies_sqli": anomalies_sqli,
        "anomalies_privilege": anomalies_privilege,
        "normal_activities": normal_activities,
        # "users_to_lock": users_to_lock_list  # List [{'user': 'abc', 'total_violation_count': 5}]
    }

    logging.info(f"Processing complete. Total anomalies: {len(anomalous_indices)} / {len(df_logs)}")
    return results


# --------------------------- Model I/O ---------------------------
def save_model_and_scaler(model, scaler, path):
    joblib.dump({'model': model, 'scaler': scaler}, path)

def load_model_and_scaler(path):
    if os.path.exists(path):
        data = joblib.load(path)
        return data['model'], data['scaler']
    return None, None