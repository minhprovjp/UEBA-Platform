# engine/realtime_engine.py
import os, json, logging, sys, signal
import time
import threading
import pandas as pd
from redis import Redis, ResponseError, ConnectionError as RedisConnectionError, RedisError
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from engine.config_manager import load_config
from engine.data_processor import load_and_process_data
from engine.db_writer import save_results_to_db
from email_alert import send_email_alert
from active_response import execute_lock_and_kill_strategy
from utils import generate_html_alert
from engine.utils import configure_redis_for_reliability, handle_redis_misconf_error
from config import *

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - [RealtimeEngine] - %(message)s")
# C·∫•u h√¨nh logging
logger = logging.getLogger("ResponseHandler")

# Flag ƒë·ªÉ ƒëi·ªÅu khi·ªÉn v√≤ng l·∫∑p
is_running = True

def handle_shutdown(signum, frame):
    """X·ª≠ l√Ω t√≠n hi·ªáu t·∫Øt (Ctrl+C) ƒë·ªÉ d·ª´ng v√≤ng l·∫∑p"""
    global is_running
    logging.info(f"üõë Nh·∫≠n t√≠n hi·ªáu d·ª´ng. ƒêang t·∫Øt Publisher...")
    is_running = False
    
# ƒêƒÉng k√Ω signal
signal.signal(signal.SIGINT, handle_shutdown)
signal.signal(signal.SIGTERM, handle_shutdown)
    
# --- H√ÄM K·∫æT N·ªêI REDIS ---
            
def connect_redis():
    while is_running:
        try:
            r = Redis.from_url(
                REDIS_URL, 
                decode_responses=True,
                socket_keepalive=True,
                socket_keepalive_options={},
                health_check_interval=30,
                retry_on_timeout=True,
                socket_connect_timeout=5
            )
            r.ping()
            
            # Configure Redis for better reliability
            configure_redis_for_reliability(r)
            
            logging.info("‚úÖ K·∫øt n·ªëi Redis th√†nh c√¥ng.")
            return r
        except Exception as e:
            logging.error(f"‚ùå L·ªói k·∫øt n·ªëi Redis: {e}. Th·ª≠ l·∫°i sau 5s...")
            time.sleep(5)
    return None

def ensure_group(r: Redis, stream: str, group: str):
    """ƒê·∫£m b·∫£o Consumer Group t·ªìn t·∫°i"""
    try:
        r.xgroup_create(stream, group, id="$", mkstream=True)
        logging.info(f"Created consumer group {group} on {stream}")
    except ResponseError as e:
        if "BUSYGROUP" in str(e):
            logging.info(f"Consumer group {group} already exists on {stream}.")
            pass
        else:
            logging.error(f"‚ùå L·ªói t·∫°o group {group} tr√™n {stream}: {e}")
            raise e


def handle_email_alerts_async(results: dict):
    """
    X·ª≠ l√Ω g·ª≠i email (Passive Response)
    """
    global LAST_EMAIL_SENT_TIME, PENDING_VIOLATIONS

    # 1. Thu th·∫≠p d·ªØ li·ªáu t√≥m t·∫Øt t·ª´ batch hi·ªán t·∫°i
    current_batch_summary = []

    # Set ƒë·ªÉ theo d√µi c√°c d√≤ng log ƒë√£ x·ª≠ l√Ω
    processed_log_indices = set()

    def add_violation_category(df, category_title):
        if df is None or df.empty:
            return

        new_logs = df[~df.index.isin(processed_log_indices)]

        if new_logs.empty:
            return

        # C·∫≠p nh·∫≠t danh s√°ch ƒë√£ x·ª≠ l√Ω
        processed_log_indices.update(new_logs.index.tolist())

        # 2. T·ªïng h·ª£p danh s√°ch c√°c Rule c·ª• th·ªÉ ƒë√£ vi ph·∫°m ƒë·ªÉ ƒë∆∞a v√†o m√¥ t·∫£
        specific_rules_desc = "Detected behaviors: "
        if 'specific_rule' in new_logs.columns:
            # L·∫•y danh s√°ch c√°c rule unique, lo·∫°i b·ªè None/R·ªóng
            unique_rules = new_logs['specific_rule'].dropna().unique().tolist()
            # L√†m s·∫°ch list
            clean_rules = set()
            for r in unique_rules:
                if r:
                    parts = [p.strip() for p in r.split(';')]
                    clean_rules.update(parts)

            if clean_rules:
                specific_rules_desc += ", ".join(sorted(list(clean_rules)))
            else:
                specific_rules_desc += "General anomaly detected"
        else:
            specific_rules_desc += "Anomaly detected (No specific rule detail)"

        # 3. Tr√≠ch xu·∫•t chi ti·∫øt User/IP (Target Aggregation)
        if 'user' in new_logs.columns and 'client_ip' in new_logs.columns:
            users_ips = new_logs.groupby(['user', 'client_ip'], observed=True).size().reset_index().apply(
                lambda x: f"{x['user']}@{x['client_ip']}", axis=1
            ).unique().tolist()
        elif 'user' in new_logs.columns:
            users_ips = new_logs['user'].unique().tolist()
        else:
            users_ips = ["Unknown"]

        # 4. X·ª≠ l√Ω th·ªùi gian (Time Range Aggregation)
        time_col = 'start_time' if 'start_time' in new_logs.columns else 'timestamp'
        first_time = new_logs[time_col].min()
        last_time = new_logs[time_col].max()

        # 5. Th√™m v√†o danh s√°ch t√≥m t·∫Øt (1 Item duy nh·∫•t cho c·∫£ nh√≥m)
        current_batch_summary.append({
            'title': category_title,  # Ti√™u ƒë·ªÅ nh√≥m (VD: TECHNICAL ATTACKS)
            'count': len(new_logs),  # T·ªïng s·ªë l∆∞·ª£ng vi ph·∫°m
            'first_time': first_time,
            'last_time': last_time,
            'desc': specific_rules_desc,  # M√¥ t·∫£ ch·ª©a danh s√°ch c√°c rule c·ª• th·ªÉ
            'targets': users_ips
        })

    # --- TH·ª® T·ª∞ G·ªåI (∆ØU TI√äN ƒê·ªò NGHI√äM TR·ªåNG) ---
    # 1. TECHNICAL ATTACKS
    add_violation_category(results.get("rule_technical"), "TECHNICAL ATTACKS")

    # 2. DATA DESTRUCTION
    add_violation_category(results.get("rule_destruction"), "DATA DESTRUCTION")

    # 3. INSIDER THREATS
    add_violation_category(results.get("rule_insider"), "INSIDER THREATS")

    # 4. ACCESS ANOMALIES
    add_violation_category(results.get("rule_access"), "ACCESS ANOMALIES")

    # 5. MULTI-TABLE ACCESS
    # add_violation_category(results.get("rule_multi_table"), "MULTI-TABLE ACCESS")

    # # 6. BEHAVIORAL ANOMALY (Profile deviation, ML)
    # add_violation_category(results.get("rule_behavior_profile"), "BEHAVIORAL ANOMALY")

    # ml_df = results.get("anomalies_ml")
    # if ml_df is not None and not ml_df.empty:
    #     ml_df = ml_df.copy()
    #     if 'specific_rule' not in ml_df.columns:
    #         ml_df['specific_rule'] = 'AI Detected Anomaly'
    #     add_violation_category(ml_df, "BEHAVIORAL ANOMALY")

    # --- LOGIC G·ª¨I THREAD (GI·ªÆ NGUY√äN) ---
    if current_batch_summary:
        PENDING_VIOLATIONS.extend(current_batch_summary)

    now = datetime.now()
    time_since_last = (now - LAST_EMAIL_SENT_TIME).total_seconds()

    if PENDING_VIOLATIONS and (time_since_last > EMAIL_COOLDOWN_SECONDS):
        data_to_send = PENDING_VIOLATIONS.copy()
        PENDING_VIOLATIONS.clear()
        LAST_EMAIL_SENT_TIME = now

        email_thread = threading.Thread(
            target=send_email_thread_worker,
            args=(data_to_send,)
        )
        email_thread.daemon = True
        email_thread.start()

def aggregate_violations(violation_list):
    """
    G·ªôp c√°c vi ph·∫°m c√πng lo·∫°i l·∫°i v·ªõi nhau.
    Input: List c√°c dict r·ªùi r·∫°c.
    Output: List c√°c dict ƒë√£ g·ªôp (Unique theo Title).
    """
    aggregated = {}

    for item in violation_list:
        title = item['title']

        if title not in aggregated:
            aggregated[title] = {
                'title': title,
                'desc': item['desc'],
                'count': 0,
                'first_time': item['first_time'],
                'last_time': item['last_time'],
                'targets': set()
            }

        # C·ªông d·ªìn
        agg = aggregated[title]
        agg['count'] += item['count']
        agg['targets'].update(item['targets'])

        # C·∫≠p nh·∫≠t th·ªùi gian min/max
        if item['first_time'] < agg['first_time']:
            agg['first_time'] = item['first_time']
        if item['last_time'] > agg['last_time']:
            agg['last_time'] = item['last_time']

    # Chuy·ªÉn ƒë·ªïi l·∫°i sang format list ƒë·ªÉ render
    final_list = []
    for val in aggregated.values():
        # Format l·∫°i th·ªùi gian v√† user list
        val['time_range'] = f"{val['first_time'].strftime('%H:%M:%S')} - {val['last_time'].strftime('%H:%M:%S')}"
        val['target_str'] = ", ".join(sorted(list(val['targets'])))
        final_list.append(val)

    return final_list

def send_email_thread_worker(summary_data):
    """H√†m worker ch·∫°y trong thread ri√™ng ƒë·ªÉ g·ª≠i email th·∫≠t."""
    try:
        # --- B∆Ø·ªöC 1: ƒê·ªåC C·∫§U H√åNH ƒê·ªòNG T·ª™ JSON ---
        # M·ªói l·∫ßn g·ª≠i mail s·∫Ω ƒë·ªçc l·∫°i file config m·ªõi nh·∫•t
        current_config = load_config()
        email_settings = current_config.get("email_alert_config", {})

        # Ki·ªÉm tra xem t√≠nh nƒÉng email c√≥ ƒë∆∞·ª£c b·∫≠t kh√¥ng
        if not email_settings.get("enable_email_alerts", True):
            logger.info("üö´ Email alerts are disabled in configuration.")
            return

        # L·∫•y th√¥ng tin ƒëƒÉng nh·∫≠p
        smtp_server = email_settings.get("smtp_server")
        smtp_port = email_settings.get("smtp_port")
        sender_email = email_settings.get("sender_email")
        sender_password = email_settings.get("sender_password")
        to_recipients = email_settings.get("to_recipients", [])
        bcc_recipients = email_settings.get("bcc_recipients", [])

        if not sender_email or not sender_password or not to_recipients:
            logger.warning("‚ö†Ô∏è Email configuration is missing in engine_config.json. Skipping alert.")
            return

        # --- B∆Ø·ªöC 2: GOM NH√ìM D·ªÆ LI·ªÜU ---
        aggregated_data = aggregate_violations(summary_data)
        
        # --- B∆Ø·ªöC 3: T·∫†O N·ªòI DUNG TEXT (Fallback) ---
        text_content = "[UEBA ALERT]: Detected abnormal behavior:\n\n"
        for item in aggregated_data:
            text_content += f"‚ö† {item['title']} ({item['count']} events)\n"
            text_content += f"   ‚Ä¢ Target: {item['target_str']}\n"
            text_content += f"   ‚Ä¢ Time: {item['time_range']}\n"
            text_content += f"   ‚Ä¢ Desc: {item['desc']}\n\n"

        text_content += "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nPlease check Dashboard for details."

        # --- B∆Ø·ªöC 4: T·∫†O N·ªòI DUNG HTML ---
        html_content = generate_html_alert(aggregated_data)

        # --- B∆Ø·ªöC 5: G·ª¨I EMAIL ---
        email_subject = f"[UEBA ALERT] Detect {len(aggregated_data)} type/s of abnormal behavior"

        # G·ªçi h√†m g·ª≠i email v·ªõi c√°c tham s·ªë l·∫•y t·ª´ config JSON
        success = send_email_alert(
            subject=email_subject,
            text_content=text_content,
            html_content=html_content,
            to_recipients=to_recipients,      # L·∫•y t·ª´ JSON
            smtp_server=smtp_server,          # L·∫•y t·ª´ JSON
            smtp_port=int(smtp_port),         # L·∫•y t·ª´ JSON (ƒë·∫£m b·∫£o l√† int)
            sender_email=sender_email,        # L·∫•y t·ª´ JSON
            sender_password=sender_password,  # L·∫•y t·ª´ JSON
            bcc_recipients=bcc_recipients     # L·∫•y t·ª´ JSON
        )

        if success is True:
            logger.info(f"--> [Security Alert Triggered] Sent successfully to {len(to_recipients)} recipients.")
        else:
            logger.error(f"--> [Security Alert] Send failed: {success}")

    except Exception as e:
        logger.error(f"--> [Security Alert] Exception error: {e}", exc_info=True)

def handle_active_responses(results: dict):
    """
    Ki·ªÉm tra danh s√°ch user v∆∞·ª£t ng∆∞·ª°ng v√† th·ª±c hi·ªán Lock/Kill.
    Args:
        results (dict): Dictionary tr·∫£ v·ªÅ t·ª´ data_processor.
    """
    users_to_lock = results.get("users_to_lock", [])

    if not users_to_lock:
        return  # Kh√¥ng c√≥ user n√†o c·∫ßn x·ª≠ l√Ω

    current_config = load_config()
    ar_config = current_config.get("active_response_config", {})
    
    # Ki·ªÉm tra c√¥ng t·∫Øc B·∫≠t/T·∫Øt
    if not ar_config.get("enable_active_response", True):
        logger.info(f"üö´ Active Response is DISABLED. Skipping action for {len(users_to_lock)} users.")
        return

    admin_user = ACTIVE_RESPONSE_SETTINGS.get('mysql_user', '')

    for offender in users_to_lock:
        user_name = offender['user']
        total_count = offender['total_violation_count']

        # === SAFETY SWITCH ===
        if admin_user and user_name == admin_user:
            logger.warning(f"‚ö†Ô∏è Detected violation on ADMIN user '{user_name}' but ignoring due to safety switch.")
            continue

        custom_reason = offender.get('lock_reason')

        if custom_reason:
            reason = f"Automatic response: {custom_reason}"
        else:
            reason = f"Automatic response: Over the threshold ({total_count})"

        try:
            execute_lock_and_kill_strategy(user_name, ACTIVE_RESPONSE_SETTINGS, reason)
        except Exception as e:
            logger.error(f"L·ªói khi th·ª±c thi Active Response cho user {user_name}: {e}")



def start_engine():
    global is_running
    
    r = connect_redis()
    
    logging.info(f"Initializing Consumer Group: {REDIS_GROUP_ENGINE}")
    for stream in STREAMS.values():
        ensure_group(r, stream, REDIS_GROUP_ENGINE)

    ensure_group(r, "uba:logs:mysql", REDIS_GROUP_ENGINE)
    logging.info("Realtime UBA Engine STARTED ‚Äî Monitoring MySQL Performance Schema")

    while is_running:
        try:
            # Check if Redis connection is still valid
            if not r:
                logging.warning("‚ö†Ô∏è Redis connection is None, reconnecting...")
                r = connect_redis()
                if not r:
                    time.sleep(5)
                    continue
            
            msgs = r.xreadgroup(
                groupname=REDIS_GROUP_ENGINE,
                consumername=REDIS_CONSUMER_NAME,
                streams=STREAMS,
                count=10000,
                block=50000
            )

            if not msgs:
                continue

            records = []
            ack_ids = []

            for stream, entries in msgs:
                for msg_id, fields in entries:
                    data = fields.get("data")
                    if data:
                        records.append(json.loads(data))
                        ack_ids.append((stream, msg_id))

            if records:
                df = pd.DataFrame(records)
                results = load_and_process_data(df, {})

                # Save to DB
                save_results_to_db(results)
                       
                try:
                    handle_email_alerts_async(results)    # Sending Alert (n·∫øu c√≥ n·ªôi dung)
                except Exception as e:
                    logging.error(f"[Email Error] Error creating email sending thread: {e}", exc_info=True)
                
                try:
                    handle_active_responses(results)   # Active Response (n·∫øu c√≥ user v∆∞·ª£t ng∆∞·ª°ng)
                except Exception as e:
                    logging.error(f"[Active Response Error] Error while executing Lock/Kill: {e}", exc_info=True)
                
                # ACK messages
                for stream, msg_id in ack_ids:
                    r.xack(stream, REDIS_GROUP_ENGINE, msg_id)

        except KeyboardInterrupt:
            logging.info("Engine stopped by user")
            break
        
        except ResponseError as e:
            # Redis stream/group errors (NOGROUP, etc.) - recreate consumer groups
            if "NOGROUP" in str(e):
                logging.warning(f"Consumer group missing: {e}")
                logging.info("üîÑ Recreating consumer groups...")
                try:
                    for stream in STREAMS.values():
                        ensure_group(r, stream, REDIS_GROUP_ENGINE)
                    ensure_group(r, "uba:logs:mysql", REDIS_GROUP_ENGINE)
                    logging.info("‚úÖ Consumer groups recreated")
                except Exception as group_error:
                    logging.error(f"Failed to recreate groups: {group_error}")
                    time.sleep(2)
            else:
                logging.error(f"Redis response error: {e}")
                time.sleep(1)
        
        except (RedisConnectionError, ConnectionResetError, BrokenPipeError) as e:
            # Redis connection errors - attempt reconnection
            logging.error(f"Redis connection error: {e}")
            logging.info("üîÑ Attempting to reconnect to Redis...")
            time.sleep(3)
            try:
                if r:
                    r.close()  # Close the broken connection
                r = connect_redis()
                if r:
                    logging.info("‚úÖ Redis reconnection successful")
                    # Re-ensure consumer groups after reconnection
                    for stream in STREAMS.values():
                        ensure_group(r, stream, REDIS_GROUP_ENGINE)
                    ensure_group(r, "uba:logs:mysql", REDIS_GROUP_ENGINE)
                else:
                    logging.error("‚ùå Redis reconnection failed, will retry...")
            except Exception as reconnect_error:
                logging.error(f"Redis reconnect error: {reconnect_error}")
        
        except Exception as e:
            # Other unexpected errors
            logging.error(f"Unexpected engine error: {e}", exc_info=True)
            time.sleep(1)

if __name__ == "__main__":
    start_engine()