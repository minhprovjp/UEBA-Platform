import pandas as pd
import os
import sqlglot
import sqlglot.expressions as exp
import sqlglot.errors as errors
import hashlib
import json
import re
from datetime import time as dt_time
import logging
import uuid
from datetime import datetime
from typing import Set
from redis import Redis, RedisError

# Th√™m c·∫•u h√¨nh logging ·ªü ƒë·∫ßu file
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [Utils] - %(message)s')

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import *

# --- Import GeoIP (X·ª≠ l√Ω n·∫øu ch∆∞a c√†i th∆∞ vi·ªán) ---
try:
    import geoip2.database
    from geopy.distance import geodesic
    HAS_GEOIP = True
except ImportError:
    HAS_GEOIP = False


# ==============================================================================
#   C√ÅC H√ÄM H·ªñ TR·ª¢ FEATURE ENGINEERING V√Ä FEEDBACK
# ==============================================================================
# C√°c h√†m trong m·ª•c n√†y h·ªó tr·ª£ vi·ªác tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho AI v√† x·ª≠ l√Ω feedback.

def get_tables_with_sqlglot(sql_query):
    """Tr√≠ch xu·∫•t t√™n c√°c b·∫£ng t·ª´ m·ªôt c√¢u l·ªánh SQL s·ª≠ d·ª•ng th∆∞ vi·ªán sqlglot."""
    tables = set() # D√πng set ƒë·ªÉ t·ª± ƒë·ªông lo·∫°i b·ªè c√°c t√™n b·∫£ng tr√πng l·∫∑p.
    
    # Tr·∫£ v·ªÅ danh s√°ch r·ªóng n·∫øu query kh√¥ng h·ª£p l·ªá.
    if pd.isna(sql_query) or not isinstance(sql_query, str) or not sql_query.strip():
        return []
        
    try:
        # Ph√¢n t√≠ch c√∫ ph√°p c√¢u l·ªánh SQL theo dialect c·ªßa MySQL.
        parsed_expression = sqlglot.parse_one(sql_query, read='mysql')
        if parsed_expression:
            # T√¨m t·∫•t c·∫£ c√°c node l√† Table trong c√¢y c√∫ ph√°p tr·ª´u t∆∞·ª£ng.
            for table_node in parsed_expression.find_all(exp.Table):
                final_table_name = table_node.name
                # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p t√™n b·∫£ng c√≥ alias.
                if hasattr(table_node, 'this') and table_node.this and isinstance(table_node.this, exp.Identifier):
                     final_table_name = table_node.this.name
                if final_table_name:
                    tables.add(final_table_name.lower()) # Chu·∫©n h√≥a v·ªÅ ch·ªØ th∆∞·ªùng.
    except (errors.ParseError, Exception):
        # B·ªè qua n·∫øu sqlglot kh√¥ng th·ªÉ ph√¢n t√≠ch c√∫ ph√°p c√¢u l·ªánh ƒë·ªÉ ch∆∞∆°ng tr√¨nh kh√¥ng b·ªã d·ª´ng.
        pass
        
    return list(tables)

def extract_query_features(sql_query):
    """
    Tr√≠ch xu·∫•t m·ªôt t·∫≠p h·ª£p c√°c ƒë·∫∑c tr∆∞ng s·ªë h·ªçc t·ª´ m·ªôt c√¢u l·ªánh SQL.
    S·ª≠ d·ª•ng sqlglot ƒë·ªÉ ph√¢n t√≠ch c√∫ ph√°p m·ªôt c√°ch hi·ªáu qu·∫£ v√† an to√†n.
    Tr·∫£ v·ªÅ m·ªôt dictionary c√°c ƒë·∫∑c tr∆∞ng s·ªë h·ªçc.
    """
    # Kh·ªüi t·∫°o gi√° tr·ªã m·∫∑c ƒë·ªãnh cho t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng
    features = {
        'num_joins': 0,
        'num_where_conditions': 0,
        'num_group_by_cols': 0,
        'num_order_by_cols': 0,
        'has_limit': 0,
        'has_subquery': 0,
        'has_union': 0,
        'has_where': 0
    }
    
    # Tr·∫£ v·ªÅ m·∫∑c ƒë·ªãnh n·∫øu query kh√¥ng h·ª£p l·ªá
    if pd.isna(sql_query) or not isinstance(sql_query, str) or not sql_query.strip():
        return features

    try:
        parsed = sqlglot.parse_one(sql_query, read='mysql')
        if parsed:
            # === K·ªπ thu·∫≠t t·ªëi ∆∞u t·ª´ code c·ªßa b·∫°n b·∫°n ===
            # ƒê·∫øm s·ªë l∆∞·ª£ng JOIN
            features['num_joins'] = sum(1 for _ in parsed.find_all(exp.Join))
            # Ki·ªÉm tra s·ª± t·ªìn t·∫°i (hi·ªáu qu·∫£ h∆°n .find is not None)
            features['has_limit'] = 1 if parsed.find(exp.Limit) else 0
            features['has_subquery'] = 1 if parsed.find(exp.Subquery) else 0
            features['has_union'] = 1 if parsed.find(exp.Union) else 0
            
            # === Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng chi ti·∫øt h∆°n ===
            # ƒê·∫øm s·ªë l∆∞·ª£ng ƒëi·ªÅu ki·ªán trong WHERE
            where_clause = parsed.find(exp.Where)
            if where_clause:
                features['has_where'] = 1
                # ∆Ø·ªõc t√≠nh s·ªë ƒëi·ªÅu ki·ªán b·∫±ng c√°ch ƒë·∫øm c√°c to√°n t·ª≠ logic
                # (AND, OR) v√† c·ªông 1. find_all nhanh h∆°n walk() cho m·ª•c ƒë√≠ch n√†y.
                # conditions = len(where_clause.find_all(exp.And, exp.Or))
                # ƒê·∫øm s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ m√† kh√¥ng c·∫ßn t·∫°o list
                conditions = sum(1 for _ in where_clause.find_all(exp.And, exp.Or))
                features['num_where_conditions'] = conditions + 1
            
            # ƒê·∫øm s·ªë c·ªôt trong GROUP BY
            group_by_clause = parsed.find(exp.Group)
            if group_by_clause:
                # `group_by_clause.expressions` l√† danh s√°ch c√°c c·ªôt
                features['num_group_by_cols'] = len(group_by_clause.expressions)

            # ƒê·∫øm s·ªë c·ªôt trong ORDER BY
            order_by_clause = parsed.find(exp.Order)
            if order_by_clause:
                features['num_order_by_cols'] = len(order_by_clause.expressions)
            
    except errors.ParseError:
        # N·∫øu sqlglot kh√¥ng parse ƒë∆∞·ª£c, gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh
        pass
    except Exception:
        # B·∫Øt c√°c l·ªói kh√¥ng l∆∞·ªùng tr∆∞·ªõc kh√°c
        pass
        
    return features

def save_feedback_to_csv(item_data: dict, label: int) -> tuple[bool, str]:
    try:
        # item_data ƒë√£ l√† dict:
        identifier_string = f"{item_data.get('timestamp')}{item_data.get('user')}{item_data.get('query')}"
        feedback_id = hashlib.md5(str(identifier_string).encode()).hexdigest()

        new_feedback_data = dict(item_data)  # clone dict
        new_feedback_data['feedback_id'] = feedback_id
        new_feedback_data['is_anomaly_label'] = label

        for k, v in list(new_feedback_data.items()):
            if isinstance(v, (list, tuple)):
                new_feedback_data[k] = json.dumps(v, ensure_ascii=False)

        # ƒë·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
        os.makedirs(os.path.dirname(FEEDBACK_FILE_PATH) or ".", exist_ok=True)

        df_feedback = pd.DataFrame()
        if os.path.exists(FEEDBACK_FILE_PATH) and os.path.getsize(FEEDBACK_FILE_PATH) > 0:
            df_feedback = pd.read_csv(FEEDBACK_FILE_PATH)

        message = ""
        if (not df_feedback.empty and
            'feedback_id' in df_feedback.columns and
            feedback_id in df_feedback['feedback_id'].values):
            message = f"ƒê√£ C·∫¨P NH·∫¨T ph·∫£n h·ªìi cho m·ª•c #{feedback_id[:8]}..."
            idx = df_feedback.index[df_feedback['feedback_id'] == feedback_id][0]
            for col, value in new_feedback_data.items():
                if col in df_feedback.columns:
                    df_feedback.loc[idx, col] = value
                else:
                    df_feedback[col] = pd.NA
                    df_feedback.loc[idx, col] = value
        else:
            message = f"ƒê√£ GHI NH·∫¨N ph·∫£n h·ªìi m·ªõi cho m·ª•c #{feedback_id[:8]}..."
            new_row_df = pd.DataFrame([new_feedback_data])
            df_feedback = pd.concat([df_feedback, new_row_df], ignore_index=True)

        ordered_cols = ['feedback_id', 'timestamp', 'user', 'query', 'is_anomaly_label']
        all_columns = sorted(list(set(df_feedback.columns.tolist() + list(new_feedback_data.keys()))))
        final_cols = [c for c in ordered_cols if c in all_columns] + [c for c in all_columns if c not in ordered_cols]

        df_feedback.to_csv(FEEDBACK_FILE_PATH, mode='w', header=True, index=False, columns=final_cols, encoding='utf-8')
        logging.info(message)
        return True, message
    except Exception as e:
        logging.error(f"ƒê√£ x·∫£y ra l·ªói khi l∆∞u ph·∫£n h·ªìi: {e}")
        import traceback; traceback.print_exc()
        return False, f"ƒê√£ x·∫£y ra l·ªói khi l∆∞u ph·∫£n h·ªìi: {e}"

        
def update_config_file(new_configs: dict):
    """
    ƒê·ªçc file config.py, t√¨m v√† thay th·∫ø c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh, v√† ghi ƒë√® l·∫°i file.

    Args:
        new_configs (dict): M·ªôt dictionary ch·ª©a c√°c gi√° tr·ªã m·ªõi c·∫ßn c·∫≠p nh·∫≠t.

    Returns:
        tuple: (bool, str) - (Th√†nh c√¥ng/Th·∫•t b·∫°i, Th√¥ng b√°o)
    """
    config_path = 'config.py' # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file config.py trong c√πng th∆∞ m·ª•c
    try:
        # ƒê·ªçc t·∫•t c·∫£ c√°c d√≤ng c·ªßa file v√†o m·ªôt danh s√°ch
        with open(config_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        new_lines = []
        # L·∫∑p qua t·ª´ng d√≤ng ƒë·ªÉ x·ª≠ l√Ω
        for line in lines:
            # D√πng regex ƒë·ªÉ t√¨m c√°c d√≤ng g√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh
            # V√≠ d·ª•: LATE_NIGHT_START_TIME_DEFAULT = time(0, 0)
            match = re.match(r'^([A-Z_]+_DEFAULT)\s*=\s*(.*)', line)
            
            # N·∫øu d√≤ng hi·ªán t·∫°i l√† m·ªôt d√≤ng g√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh
            if match:
                var_name = match.group(1) # L·∫•y t√™n bi·∫øn, v√≠ d·ª•: LATE_NIGHT_START_TIME_DEFAULT
                
                # N·∫øu bi·∫øn n√†y c√≥ trong danh s√°ch c·∫ßn c·∫≠p nh·∫≠t
                if var_name in new_configs:
                    new_value = new_configs[var_name]
                    
                    # ƒê·ªãnh d·∫°ng l·∫°i gi√° tr·ªã m·ªõi th√†nh m·ªôt chu·ªói Python h·ª£p l·ªá
                    if isinstance(new_value, str):
                        # Chu·ªói ph·∫£i ƒë∆∞·ª£c ƒë·∫∑t trong d·∫•u ngo·∫∑c k√©p
                        new_line = f'{var_name} = r"{new_value}"\n' if '\\' in new_value else f'{var_name} = "{new_value}"\n'
                    elif isinstance(new_value, dt_time):
                        # ƒê·ªëi t∆∞·ª£ng time c·∫ßn ƒë∆∞·ª£c t√°i t·∫°o b·∫±ng time(...)
                        new_line = f"{var_name} = dt_time({new_value.hour}, {new_value.minute}, {new_value.second})\n"
                    elif isinstance(new_value, list):
                        # str(my_list) s·∫Ω t·∫°o ra m·ªôt chu·ªói nh∆∞ "['item1', 'item2']" tr√™n m·ªôt d√≤ng.
                        new_line = f"{var_name} = {str(new_value)}\n"
                    else:
                        # C√°c ki·ªÉu d·ªØ li·ªáu kh√°c (int, float)
                        new_line = f'{var_name} = {new_value}\n'
                    
                    # Th√™m d√≤ng m·ªõi ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng v√†o danh s√°ch `new_lines`
                    new_lines.append(new_line)
                    print(f"ƒêang c·∫≠p nh·∫≠t {var_name}...")
                else:
                    # N·∫øu bi·∫øn n√†y kh√¥ng c·∫ßn c·∫≠p nh·∫≠t, gi·ªØ nguy√™n d√≤ng c≈©
                    new_lines.append(line)
            else:
                # Gi·ªØ nguy√™n c√°c d√≤ng kh√¥ng ph·∫£i l√† d√≤ng g√°n gi√° tr·ªã (v√≠ d·ª•: comment, import,...)
                new_lines.append(line)

        # Ghi ƒë√® l·∫°i to√†n b·ªô file config.py v·ªõi n·ªôi dung m·ªõi
        # Ch·∫ø ƒë·ªô 'w' (write) s·∫Ω t·ª± ƒë·ªông x√≥a n·ªôi dung c≈© tr∆∞·ªõc khi ghi
        with open(config_path, 'w', encoding='utf-8') as f:
            f.writelines(new_lines)
            
        return True, "L∆∞u c·∫•u h√¨nh m·∫∑c ƒë·ªãnh th√†nh c√¥ng!"

    except Exception as e:
        # B·∫Øt l·ªói v√† in ra ƒë·ªÉ d·ªÖ d√†ng g·ª° l·ªói
        import traceback
        traceback.print_exc()
        return False, f"L·ªói khi l∆∞u c·∫•u h√¨nh: {e}"


def save_logs_to_parquet(records: list, source_dbms: str) -> int:
    if not records:
        return 0
    try:
        df = pd.DataFrame(records)
        if 'source_dbms' not in df.columns:
            df['source_dbms'] = source_dbms
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)

        os.makedirs(STAGING_DATA_DIR, exist_ok=True)  # <-- th√™m d√≤ng n√†y

        filename = f"{source_dbms}_{datetime.now().strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex[:8]}.parquet"
        file_path = os.path.join(STAGING_DATA_DIR, filename)
        df.to_parquet(file_path, engine='pyarrow', index=False)
        logging.info(f"ƒê√£ l∆∞u {len(df)} b·∫£n ghi t·ª´ '{source_dbms}' v√†o file: {filename}")
        return len(df)
    except Exception as e:
        logging.error(f"L·ªói khi l∆∞u file Parquet: {e}")
        return 0

def get_normalized_query(query: str) -> str:
    """Extract DIGEST_TEXT-like normalized query"""
    if not query:
        return ""
    # Simple normalization (you can use sqlglot for better)
    query = re.sub(r'"\w+"', '"?"', query)
    query = re.sub(r"'\w+'", "'?'", query)
    query = re.sub(r'\d+', '?', query)
    return query.strip()

def count_sensitive_tables(tables: list) -> int:
    if not tables:
        return 0
    return len([t for t in tables if any(st in t.lower() for st in SENSITIVE_TABLES)])

def is_late_night(ts):
    from datetime import datetime
    if isinstance(ts, str):
        ts = datetime.fromisoformat(ts.replace("Z", "+00:00"))
    return ts.hour <= 5 or ts.hour >= 23


# ============================================================
#   ACTIVE RESPONSE AUDIT LOGGER
# ============================================================

# C·∫•u h√¨nh logger
audit_logger = logging.getLogger('ActiveResponseAudit')
audit_logger.setLevel(logging.INFO)
audit_logger.propagate = False

# Ch·ªâ th√™m handler n·∫øu n√≥ ch∆∞a c√≥ ƒë·ªÉ tr√°nh log l·∫∑p l·∫°i
if not audit_logger.hasHandlers():
    try:
        # S·ª≠ d·ª•ng 'a' ƒë·ªÉ ghi n·ªëi ti·∫øp, 'utf-8'
        file_handler = logging.FileHandler(ACTIVE_RESPONSE_AUDIT_LOG_PATH, mode='a', encoding='utf-8')
        # ƒê·ªãnh d·∫°ng log: th·ªùi gian v√† n·ªôi dung
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
        audit_logger.addHandler(file_handler)
    except Exception as e:
        print(f"L·ªñI: Kh√¥ng th·ªÉ t·∫°o file audit log t·∫°i {ACTIVE_RESPONSE_AUDIT_LOG_PATH}: {e}")

def log_active_response_action(action: str, target: str, reason: str):
    """
    Ghi l·∫°i m·ªôt h√†nh ƒë·ªông ph·∫£n ·ª©ng ch·ªß ƒë·ªông v√†o file audit log.

    Args:
        action (str): Lo·∫°i h√†nh ƒë·ªông (v√≠ d·ª•: "LOCK_ACCOUNT", "KILL_SESSION").
        target (str): ƒê·ªëi t∆∞·ª£ng b·ªã t√°c ƒë·ªông (v√≠ d·ª•: "user@host", "Session 123").
        reason (str): L√Ω do th·ª±c hi·ªán.
    """
    try:
        message = f"ACTION: {action} | TARGET: {target} | REASON: {reason}"
        audit_logger.info(message)
        for handler in audit_logger.handlers:
            handler.flush()
    except Exception as e:
        print(f"[Active Response] L·ªói khi ghi audit log: {e}")

def generate_html_alert(violation_summary: list):
    """
    T·∫°o n·ªôi dung HTML cho email c·∫£nh b√°o.
    Args:
        violation_summary: List c√°c dict [{'title': 'Gi·ªù Khuya', 'count': 4, 'time': '...', 'desc': '...'}]
    """

    # CSS Inline
    html_template = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <style>
            body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; }}
            .container {{ max_width: 600px; margin: 0 auto; padding: 20px; background-color: #f9f9f9; }}
            .header {{ background-color: #d32f2f; color: white; padding: 20px; text-align: center; border-radius: 5px 5px 0 0; }}
            .alert-box {{ background-color: #fff; padding: 20px; border: 1px solid #ddd; border-top: none; border-radius: 0 0 5px 5px; }}
            .stat-box {{ background-color: #fff3cd; border-left: 5px solid #ffc107; padding: 15px; margin-bottom: 20px; }}
            table {{ width: 100%; border-collapse: collapse; margin-top: 20px; }}
            th {{ background-color: #f2f2f2; text-align: left; padding: 10px; border-bottom: 2px solid #ddd; font-size: 12px; text-transform: uppercase; color: #555; }}
            td {{ padding: 12px 10px; border-bottom: 1px solid #eee; font-size: 14px; }}
            .severity-high {{ color: #d32f2f; font-weight: bold; }}
            .footer {{ text-align: center; font-size: 12px; color: #777; margin-top: 20px; }}
            .btn {{ display: inline-block; background-color: #d32f2f; color: white; padding: 10px 20px; text-decoration: none; border-radius: 3px; margin-top: 20px; font-weight: bold; }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h2 style="margin:0;">üö® Security Alert Triggered</h2>
                <p style="margin:5px 0 0 0; font-size: 14px;">UEBA Detection System</p>
            </div>

            <div class="alert-box">
                <p>The UEBA system has detected abnormal behaviors that require your attention.</p>

                <div class="stat-box">
                    <strong>Overview:</strong> Detected <strong>{len(violation_summary)}</strong> type/s of anomalies in the latest scan.
                </div>

                <table width="100%">
                    <thead>
                        <tr>
                            <th>Anomaly Type</th>
                            <th style="text-align: center;">Count</th>
                            <th>Entity (User@IP)</th>
                            <th>Occurrence (First - Last)</th>
                        </tr>
                    </thead>
                    <tbody>
    """

    # Loop ƒë·ªÉ t·∫°o c√°c d√≤ng trong b·∫£ng
    for item in violation_summary:
        html_template += f"""
            <tr>
                <td>
                    <span class="severity-high">{item['title']}</span><br>
                    <span style="font-size: 11px; color: #777;">{item['desc']}</span>
                </td>
                <td style="text-align: center;"><strong>{item['count']}</strong></td>
                <td style="font-size: 13px; color: #333;">
                    {item.get('target_str', 'N/A')}
                </td>
                <td style="font-size: 13px; white-space: nowrap;">
                    {item.get('time_range', 'N/A')}
                </td>
            </tr>
        """
    html_template += """
                    </tbody>
                </table>

            </div>

            <div class="footer" style="
                margin-top: 25px;
                text-align: center;
                background: #f5f5f5;
                padding: 15px 10px;
                border-top: 2px solid #d0d0d0;
                font-size: 12px;
                font-style: italic;
                color: #555;
            ">
                <p>
                    This is an automated email from the UEBA Platform system.<br>
                    Please do not reply to this email.
                </p>
            </div>

        </div>
    </body>
    </html>
    """
    return html_template

# ==============================================================================
#   C√ÅC H√ÄM H·ªñ TR·ª¢ PH√ÇN T√çCH THEO LU·∫¨T (RULE-BASED ANALYSIS)
# ==============================================================================

# ==============================================================================
# 1. NH√ìM ACCESS ANOMALIES (B·∫•t th∆∞·ªùng truy c·∫≠p)
# Bao g·ªìm: Concurrent Login, Brute-force, Impossible Travel
# ==============================================================================
def check_access_anomalies(df, rule_config):
    """Nh√≥m 1: B·∫•t th∆∞·ªùng truy c·∫≠p"""
    anomalies = {}
    thresholds = rule_config.get('thresholds', {})
    settings = rule_config.get('settings', {})
    
    # Rule 1. Concurrent Login
    limit_ips = thresholds.get('concurrent_ips_limit', 1)
    df_sorted = df.sort_values('timestamp')
    grouped = df_sorted.groupby(['user', pd.Grouper(key='timestamp', freq='5Min')], observed=False)
    idx_concurrent = []
    for (user, time), group in grouped:
        if group['client_ip'].nunique() > limit_ips:
            idx_concurrent.extend(group.index.tolist()) 
    if idx_concurrent:
        anomalies['Concurrent Login'] = list(set(idx_concurrent))
                
    # Rule 2. Brute-force Success
    limit_attempts = thresholds.get('brute_force_attempts', 5)
    failed_attempts = df[df['error_code'] != 0] # Error code != 0 l√† l·ªói
    idx_bruteforce = [] 
    if not failed_attempts.empty:
        ip_error_counts = failed_attempts.groupby(['client_ip', pd.Grouper(key='timestamp', freq='1Min')], observed=False).size()
        suspicious_ips = ip_error_counts[ip_error_counts > limit_attempts].index.get_level_values(0).unique()     
        # T√¨m log th√†nh c√¥ng ngay sau chu·ªói l·ªói t·ª´ IP ƒë√≥
        brute_force_success = df[
            (df['client_ip'].isin(suspicious_ips)) & 
            (df['error_code'] == 0) & 
            (df['event_name'] == 'connect')
        ]
        idx_bruteforce.extend(brute_force_success.index.tolist())      
    if idx_bruteforce:
        anomalies['Brute-force Success'] = list(set(idx_bruteforce))
        
    # Rule 3. Impossible Travel
    if HAS_GEOIP:
        max_speed = thresholds.get('impossible_travel_speed_kmh', 800)
        db_path = settings.get('geoip_db_path', 'engine/geoip/GeoLite2-City.mmdb')
        idx_travel = []  
        if os.path.exists(db_path):        
            try:
                reader = geoip2.database.Reader(db_path)
                def get_lat_lon(ip):
                    try:
                        if ip in ['127.0.0.1', 'localhost', '::1', '0.0.0.0']: return None
                        res = reader.city(ip)
                        return (res.location.latitude, res.location.longitude)
                    except: return None              
                # Group by User ƒë·ªÉ check h√†nh tr√¨nh
                grouped_user = df_sorted.groupby('user', observed=False)
                for user, group in grouped_user:
                    if len(group) < 2: continue
                    prev_row = None
                    for idx, row in group.iterrows():
                        curr_loc = get_lat_lon(row['client_ip'])
                        if prev_row and curr_loc and prev_row['loc']:
                            dist = geodesic(prev_row['loc'], curr_loc).km
                            time_diff = (row['timestamp'] - prev_row['time']).total_seconds() / 3600
                            if time_diff > 0 and dist > 50:
                                speed = dist / time_diff
                                if speed > max_speed:
                                    idx_travel.append(idx)
                        if curr_loc:
                            prev_row = {'loc': curr_loc, 'time': row['timestamp']}
                reader.close()
            except Exception as e:
                logging.error(f"GeoIP Logic Error: {e}")             
        if idx_travel:
            anomalies['Impossible Travel'] = list(set(idx_travel))

    return anomalies

# ==============================================================================
# 2. NH√ìM INSIDER THREATS (M·ªëi ƒëe d·ªça n·ªôi b·ªô)
# Bao g·ªìm: Service Account, Admin Privilege Escalation, Sensitive Access, Late Night, Ghost Account
# ==============================================================================
def check_insider_threats(df, rule_config):
    """Nh√≥m 2: Insider Threat"""
    anomalies = {}
    service_accounts = rule_config.get('service_accounts', {})
    signatures = rule_config.get('signatures', {})
    settings = rule_config.get('settings', {})
    hr_users = set(signatures.get('hr_authorized_users', []))
    
    # Rule 4. Service Account Misuse
    idx_service = []
    for user, config in service_accounts.items():
        user_logs = df[df['user'] == user]
        if user_logs.empty: continue
        invalid_hour = user_logs[~user_logs['timestamp'].dt.hour.isin(config.get('allowed_hours', []))]
        idx_service.extend(invalid_hour.index.tolist())  
        invalid_ip = user_logs[~user_logs['client_ip'].isin(config.get('allowed_ips', []))]
        idx_service.extend(invalid_ip.index.tolist())      
    if idx_service: anomalies['Service Account Misuse'] = list(set(idx_service))

    # Rule 5. Admin Privilege Escalation
    idx_admin = []
    admin_kws = signatures.get('admin_keywords', [])
    if admin_kws:
        pattern = "|".join(re.escape(k) for k in admin_kws)
        admin_actions = df[
            (df['query'].str.contains(pattern, case=False, na=False)) &
            (df['user'] != 'root')
        ]
        idx_admin.extend(admin_actions.index.tolist())       
    if idx_admin: anomalies['Admin Privilege Abuse'] = list(set(idx_admin))
        
    # Rule 6. Sensitive Table Access 
    idx_sensitive = []
    sensitive_tables = signatures.get('sensitive_tables', [])
    allowed_users = settings.get('sensitive_allowed_users', [])    
    # H√†m check logic
    def is_violation(row):
        # N·∫øu user n·∫±m trong whitelist th√¨ b·ªè qua
        if row['user'] in allowed_users: return False
        
        # Check n·∫øu query ch·ª©a b·∫£ng nh·∫°y c·∫£m
        for tbl in sensitive_tables:
            if tbl in str(row['query']):
                return True # Vi ph·∫°m: User kh√¥ng ƒë∆∞·ª£c ph√©p truy c·∫≠p b·∫£ng n√†y
        return False
    if sensitive_tables:
        sensitive_violation = df[df.apply(is_violation, axis=1)]
        idx_sensitive.extend(sensitive_violation.index.tolist())   
    if idx_sensitive: anomalies['Sensitive Table Access'] = list(set(idx_sensitive))

    # Rule 7. Late Night Query 
    # Logic: Truy c·∫≠p ngo√†i gi·ªù h√†nh ch√≠nh (22h - 5h s√°ng)
    idx_latenight = []
    try:
        s_str = settings.get('late_night_start', '22:00:00')
        e_str = settings.get('late_night_end', '05:00:00')
        start_time = dt_time.fromisoformat(s_str)
        end_time = dt_time.fromisoformat(e_str)
        def check_time(ts):
            t = ts.time()
            if start_time <= end_time: return start_time <= t <= end_time
            else: return start_time <= t or t <= end_time # Qua ƒë√™m (v√≠ d·ª• 22h -> 5h)
        late_night_logs = df[df['timestamp'].apply(check_time)]
        # C√≥ th·ªÉ l·ªçc b·ªõt c√°c user ch·∫°y job ƒë√™m n·∫øu c·∫ßn
        idx_latenight.extend(late_night_logs.index.tolist())
    except:
        pass 
    if idx_latenight: anomalies['Late Night Query'] = list(set(idx_latenight))
    
    # Rule 8. Ghost Account Creation
    idx_ghost = []
    create_cmds = df[df['query'].str.contains("CREATE USER", case=False, na=False)]
    if not create_cmds.empty:
        pattern = re.compile(r"CREATE\s+USER\s+['\"`]?([a-zA-Z0-9_]+)['\"`]?", re.IGNORECASE)
        for idx, row in create_cmds.iterrows():
            match = pattern.search(row['query'])
            if match and match.group(1) not in hr_users:
                idx_ghost.append(idx)               
    if idx_ghost: anomalies['Ghost Account Creation'] = list(set(idx_ghost))
    
    return anomalies

# ==============================================================================
# 3. NH√ìM TECHNICAL ATTACKS (T·∫•n c√¥ng k·ªπ thu·∫≠t)
# Bao g·ªìm: SQLi, DoS, High CPU Usage, Scan Efficiency, Config Change, Entropy, Client Mismatch
# ==============================================================================
def check_technical_attacks(df, rule_config):
    """Nh√≥m 3: Technical Attacks"""
    anomalies = {}
    thresholds = rule_config.get('thresholds', {})
    signatures = rule_config.get('signatures', {})
    
    # Rule 9. SQL Injection
    idx_sqli = []
    sqli_kws = signatures.get('sqli_keywords', [])
    if sqli_kws:
        pattern_sqli = "|".join(re.escape(k) for k in sqli_kws)
        sqli_logs = df[df['query'].str.contains(pattern_sqli, case=False, na=False)]
        idx_sqli.extend(sqli_logs.index.tolist())
    if idx_sqli: anomalies['SQL Injection'] = list(set(idx_sqli))
    
    # Rule 10. DoS / Resource Exhaustion
    idx_dos = []
    max_time = thresholds.get('execution_time_limit_ms', 5000)
    idx_dos.extend(df[df['execution_time_ms'] > max_time].index.tolist())
    if idx_dos: anomalies['DoS / Resource Exhaustion'] = list(set(idx_dos))
    
    # Rule 11. High CPU Usage 
    idx_cpu = []
    max_cpu_time = thresholds.get('cpu_time_limit_ms', 1000) # 1s CPU
    if 'cpu_time_ms' in df.columns:
        high_cpu = df[df['cpu_time_ms'] > max_cpu_time]
        idx_cpu.extend(high_cpu.index.tolist())
    if idx_cpu: anomalies['High CPU Usage'] = list(set(idx_cpu))
    
    # Rule 12. Scan Efficiency
    idx_scan = []
    min_eff = thresholds.get('scan_efficiency_min', 0.01)
    min_rows = thresholds.get('scan_efficiency_min_rows', 1000)
    inefficient = df[
        (df['rows_examined'] > min_rows) & 
        (df['rows_returned'] < (df['rows_examined'] * min_eff))
    ]
    idx_scan.extend(inefficient.index.tolist())
    if idx_scan: anomalies['Scan Efficiency'] = list(set(idx_scan))
    
    # Rule 13. Config Change
    idx_config = []
    config_change = df[
        df['query'].str.contains("SET GLOBAL|general_log", regex=True, case=False, na=False)
    ]
    idx_config.extend(config_change.index.tolist())
    if idx_config: anomalies['Config Change'] = list(set(idx_config))
    
    # Rule 14. High Entropy Queries
    idx_entropy = []
    max_entropy = thresholds.get('max_query_entropy', 4.8)
    if 'query_entropy' in df.columns:
        high_entropy = df[df['query_entropy'] > max_entropy]
        idx_entropy.extend(high_entropy.index.tolist())
    if idx_entropy: anomalies['High Entropy Query'] = list(set(idx_entropy))

    # Rule 15. Client/OS Mismatch
    # Ph√°t hi·ªán tool t·∫•n c√¥ng trong blacklist (sqlmap, nmap...)
    idx_client = []
    whitelist = signatures.get('allowed_programs', [])
    if 'program_name' in df.columns and not whitelist:
        pattern_bad = "|".join(re.escape(p) for p in whitelist)
        bad_clients = df[df['program_name'].str.contains(pattern_bad, case=False, na=False)]
        idx_client.extend(bad_clients.index.tolist())
    if idx_client: anomalies['Client Mismatch'] = list(set(idx_client))

    return anomalies

# ==============================================================================
# 4. NH√ìM DATA DESTRUCTION (Ph√° ho·∫°i d·ªØ li·ªáu)
# Bao g·ªìm: Mass Delete, Old Data, Large Dump
# ==============================================================================
def check_data_destruction(df, rule_config):
    """Nh√≥m 4: Data Destruction"""
    anomalies = {}
    thresholds = rule_config.get('thresholds', {})
    signatures = rule_config.get('signatures', {})
    
    # Rule 16. Mass Deletion (X√≥a h√†ng lo·∫°t)
    idx_delete = []
    limit_rows = thresholds.get('mass_deletion_rows', 500)
    mass_delete = df[
        (df['event_name'].isin(['statement/sql/delete', 'statement/sql/drop'])) &
        (df['rows_affected'] > limit_rows)
    ]
    idx_delete.extend(mass_delete.index.tolist())
    if idx_delete: anomalies['Mass Deletion'] = list(set(idx_delete))
    
    # Rule 17. Old Data Modification (S·ª≠a d·ªØ li·ªáu c≈©)
    idx_old_data = []
    old_data_access = df[df['query'].str.contains("2019|2020|2021|2022|2023|2024", regex=True, na=False)]
    idx_old_data.extend(old_data_access.index.tolist())
    if idx_old_data: anomalies['Old Data Modification'] = list(set(idx_old_data))

    # Rule 18. Large Data Dump 
    # Logic: Select b·∫£ng quan tr·ªçng m√† tr·∫£ v·ªÅ qu√° nhi·ªÅu d√≤ng
    idx_dump = []
    large_dump_tables = signatures.get('large_dump_tables', [])
    if large_dump_tables:
        pattern_dump = "|".join(re.escape(k) for k in large_dump_tables)
        # ƒêi·ªÅu ki·ªán: Query ch·ª©a t√™n b·∫£ng quan tr·ªçng V√Ä tr·∫£ v·ªÅ > 1000 d√≤ng
        dump_logs = df[
            (df['query'].str.contains(pattern_dump, case=False, na=False)) &
            (df['rows_returned'] > 1000)
        ]
        idx_dump.extend(dump_logs.index.tolist())
    if idx_dump: anomalies['Large Data Dump'] = list(set(idx_dump))
    
    return anomalies

# ==============================================================================
# 5. RULE RI√äNG: MULTI-TABLE ACCESS 
# ==============================================================================
def check_multi_table_anomalies(df, rule_config):
    """
    Rule 19: Multi-table Access
    """
    anomalies = []
    thresholds = rule_config.get('thresholds', {})
    
    # L·∫•y tham s·ªë (M·∫∑c ƒë·ªãnh 5 ph√∫t, 3 b·∫£ng)
    window_min = thresholds.get('multi_table_window_minutes', 5)
    min_tables = thresholds.get('multi_table_min_count', 3)
    
    # H√†m t√°ch t√™n b·∫£ng t·ª´ c√¢u l·ªánh SQL
    def extract_table_name(q):
        if not isinstance(q, str): return None
        # Regex t√¨m t·ª´ sau FROM ho·∫∑c JOIN
        match = re.search(r'\b(?:FROM|JOIN)\s+[`\'"]?([a-zA-Z0-9_.]+)[`\'"]?', q, re.IGNORECASE)
        if match:
            # Lo·∫°i b·ªè k√Ω t·ª± quote th·ª´a n·∫øu c√≥ (v√≠ d·ª•: `sales_db`.`orders` -> sales_db.orders)
            return match.group(1).replace('`', '').replace("'", "").replace('"', "")
        return None

    # Ch·ªâ x√©t c√°c c√¢u l·ªánh SELECT
    df_select = df[df['query'].str.contains('SELECT', case=False, na=False)].copy()
    if df_select.empty: return []

    # T·∫°o c·ªôt t√™n b·∫£ng t·∫°m th·ªùi
    df_select['extracted_table'] = df_select['query'].apply(extract_table_name)
    # L·ªçc b·ªè c√°c gi√° tr·ªã None ho·∫∑c r·ªóng
    df_select = df_select[df_select['extracted_table'].notna() & (df_select['extracted_table'] != '')]

    # Group theo User v√† Khung gi·ªù
    # Sort tr∆∞·ªõc ƒë·ªÉ Grouper ch·∫°y ƒë√∫ng
    df_select = df_select.sort_values('timestamp')
    grouped = df_select.groupby(['user', pd.Grouper(key='timestamp', freq=f'{window_min}Min')], observed=False)

    for (user, time_window), group in grouped:
        unique_tables = group['extracted_table'].nunique()
        if unique_tables > min_tables:
            # --- ƒêI·ªÇM S·ª¨A QUAN TR·ªåNG: Tr·∫£ v·ªÅ to√†n b·ªô index c·ªßa nh√≥m n√†y ---
            anomalies.extend(group.index.tolist())

    return list(set(anomalies))

# ==============================================================================
# 5. RULE RI√äNG: BEHAVIORAL PROFILE
# ==============================================================================
def check_unusual_user_activity_time(row, profiles):
    """
    Rule 20: Unusual User Time
    """
    user = row.get('user')
    if user not in profiles: return None
    
    hour = row['timestamp'].hour
    p = profiles[user]
    # Gi·∫£ s·ª≠ profile l∆∞u 'active_start' v√† 'active_end'
    # N·∫øu gi·ªù n·∫±m ngo√†i kho·∫£ng [start - 1, end + 1] th√¨ b√°o
    if hour < (p['active_start'] - 1) or hour > (p['active_end'] + 1):
        return f"User {user} active at {hour}h (Profile: {p['active_start']}-{p['active_end']})"
    return None

# ==============================================================================
# REDIS CONFIGURATION HELPER
# ==============================================================================

def configure_redis_for_reliability(redis_client: Redis) -> bool:
    """
    Configure Redis for better reliability and handle MISCONF errors.
    
    Returns:
        bool: True if configuration was successful, False otherwise
    """
    try:
        # Strategy 1: Use AOF instead of RDB for better persistence
        try:
            redis_client.config_set("save", "")  # Disable RDB snapshots
            logging.info("‚úÖ Redis: Disabled RDB snapshots")
            
            redis_client.config_set("appendonly", "yes")  # Enable AOF
            logging.info("‚úÖ Redis: Enabled AOF persistence")
            
            return True
            
        except Exception as config_error:
            logging.warning(f"‚ö†Ô∏è Could not configure Redis persistence: {config_error}")
            
            # Strategy 2: Fallback - disable the error check
            try:
                redis_client.config_set("stop-writes-on-bgsave-error", "no")
                logging.warning("‚ö†Ô∏è Redis: Disabled RDB error checking (fallback)")
                return True
                
            except Exception as fallback_error:
                logging.warning(f"‚ö†Ô∏è Redis: Could not modify config: {fallback_error}")
                return False
                
    except Exception as e:
        logging.error(f"‚ùå Redis configuration failed: {e}")
        return False

def handle_redis_misconf_error(error_msg: str) -> str:
    """
    Provide helpful error message and suggestions for MISCONF errors.
    
    Args:
        error_msg: The Redis error message
        
    Returns:
        str: Helpful suggestion message
    """
    if "MISCONF" in error_msg:
        return (
            "üí° Redis MISCONF Error Solutions:\n"
            "   1. Check disk space: df -h\n"
            "   2. Check Redis logs: tail -f /var/log/redis/redis-server.log\n"
            "   3. Fix permissions: sudo chown redis:redis /var/lib/redis\n"
            "   4. Disable RDB: redis-cli CONFIG SET save ''\n"
            "   5. Enable AOF: redis-cli CONFIG SET appendonly yes"
        )
    return "Check Redis server status and logs"