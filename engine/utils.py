import pandas as pd
import os
import sqlglot
import sqlglot.errors as errors
import hashlib
import json
import re
from datetime import time as dt_time
import logging
import uuid
from datetime import datetime
from typing import Set
from redis import Redis, RedisError

# Th√™m c·∫•u h√¨nh logging ·ªü ƒë·∫ßu file
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [Utils] - %(message)s')

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import *

# --- Import GeoIP (X·ª≠ l√Ω n·∫øu ch∆∞a c√†i th∆∞ vi·ªán) ---
try:
    import geoip2.database
    from geopy.distance import geodesic
    HAS_GEOIP = True
except ImportError:
    HAS_GEOIP = False

try:
    import sqlglot
    from sqlglot import exp
    SQLGLOT_AVAILABLE = True
except ImportError:
    SQLGLOT_AVAILABLE = False
    
# ==============================================================================
#   C√ÅC H√ÄM H·ªñ TR·ª¢ FEATURE ENGINEERING V√Ä FEEDBACK
# ==============================================================================
# C√°c h√†m trong m·ª•c n√†y h·ªó tr·ª£ vi·ªác tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho AI v√† x·ª≠ l√Ω feedback.

def extract_db_from_sql(sql_text):
    """
    T·ª± ƒë·ªông tr√≠ch xu·∫•t t√™n Database t·ª´ c√¢u l·ªánh SQL.
    ∆Øu ti√™n d√πng SQLGlot, fallback sang Regex.
    V√≠ d·ª•: "SELECT * FROM sales_db.orders" -> "sales_db"
    
    Improvements:
    - Better regex patterns to catch more cases
    - Handle multiple databases in one query (returns first non-system DB)
    - Better error handling and logging
    - Support for more SQL statement types
    """
    if not sql_text or not isinstance(sql_text, str):
        return None
    
    # Clean and normalize the SQL text
    sql_text = sql_text.strip()
    if not sql_text:
        return None

    # C√°ch 1: D√πng SQLGlot (Ch√≠nh x√°c nh·∫•t)
    if SQLGLOT_AVAILABLE:
        try:
            # Parse query (limit ƒë·ªô d√†i ƒë·ªÉ tr√°nh treo)
            parsed = sqlglot.parse_one(sql_text[:5000], read="mysql")
            if parsed:
                # Collect all database names, prioritize non-system databases
                databases = []
                for table in parsed.find_all(exp.Table):
                    if table.db and table.db.lower() not in ['mysql', 'sys', 'information_schema', 'performance_schema']:
                        databases.append(table.db.lower())
                    elif table.db:  # System database as fallback
                        databases.append(table.db.lower())
                
                # Return first non-system database, or first database if all are system
                if databases:
                    return databases[0]
        except Exception as e:
            # Log parsing errors for debugging but continue with regex fallback
            logging.debug(f"SQLGlot parsing failed for query: {e}")

    # C√°ch 2: D√πng Regex (Nhanh, d·ª± ph√≤ng) - Improved patterns
    # Enhanced regex patterns to catch more SQL statement types and formats
    patterns = [
        # Standard table references with database prefix
        r'(?:FROM|JOIN|UPDATE|INTO|TABLE|REPLACE\s+INTO)\s+[`\'"]?([a-zA-Z0-9_]+)[`\'"]?\s*\.\s*[`\'"]?[a-zA-Z0-9_]+[`\'"]?',
        
        # USE statement
        r'USE\s+[`\'"]?([a-zA-Z0-9_]+)[`\'"]?(?:\s|;|$)',
        
        # CREATE/DROP/ALTER DATABASE
        r'(?:CREATE|DROP|ALTER)\s+(?:DATABASE|SCHEMA)\s+(?:IF\s+(?:NOT\s+)?EXISTS\s+)?[`\'"]?([a-zA-Z0-9_]+)[`\'"]?',
        
        # SHOW statements with database context
        r'SHOW\s+(?:TABLES|COLUMNS|INDEX)\s+(?:FROM|IN)\s+[`\'"]?([a-zA-Z0-9_]+)[`\'"]?',
        
        # INSERT INTO with database prefix
        r'INSERT\s+(?:IGNORE\s+)?INTO\s+[`\'"]?([a-zA-Z0-9_]+)[`\'"]?\s*\.\s*[`\'"]?[a-zA-Z0-9_]+[`\'"]?',
        
        # DELETE FROM with database prefix
        r'DELETE\s+FROM\s+[`\'"]?([a-zA-Z0-9_]+)[`\'"]?\s*\.\s*[`\'"]?[a-zA-Z0-9_]+[`\'"]?',
        
        # CALL stored procedure with database prefix
        r'CALL\s+[`\'"]?([a-zA-Z0-9_]+)[`\'"]?\s*\.\s*[`\'"]?[a-zA-Z0-9_]+[`\'"]?',
        
        # DESCRIBE/DESC with database prefix
        r'(?:DESCRIBE|DESC)\s+[`\'"]?([a-zA-Z0-9_]+)[`\'"]?\s*\.\s*[`\'"]?[a-zA-Z0-9_]+[`\'"]?'
    ]
    
    # Collect all matches and prioritize non-system databases
    found_databases = []
    system_databases = {'mysql', 'sys', 'information_schema', 'performance_schema'}
    
    for pattern in patterns:
        matches = re.finditer(pattern, sql_text, re.IGNORECASE)
        for match in matches:
            db_name = match.group(1).lower()
            if db_name not in system_databases:
                found_databases.append(db_name)
            elif not found_databases:  # Only add system DB if no user DB found yet
                found_databases.append(db_name)
    
    # Return first non-system database, or first database if all are system
    if found_databases:
        return found_databases[0]

    return None

def get_tables_with_sqlglot(sql_query):
    """Tr√≠ch xu·∫•t t√™n c√°c b·∫£ng t·ª´ m·ªôt c√¢u l·ªánh SQL s·ª≠ d·ª•ng th∆∞ vi·ªán sqlglot."""
    tables = set() # D√πng set ƒë·ªÉ t·ª± ƒë·ªông lo·∫°i b·ªè c√°c t√™n b·∫£ng tr√πng l·∫∑p.
    
    # Tr·∫£ v·ªÅ danh s√°ch r·ªóng n·∫øu query kh√¥ng h·ª£p l·ªá.
    if pd.isna(sql_query) or not isinstance(sql_query, str) or not sql_query.strip():
        return []
        
    try:
        # Ph√¢n t√≠ch c√∫ ph√°p c√¢u l·ªánh SQL theo dialect c·ªßa MySQL.
        parsed_expression = sqlglot.parse_one(sql_query, read='mysql')
        if parsed_expression:
            # T√¨m t·∫•t c·∫£ c√°c node l√† Table trong c√¢y c√∫ ph√°p tr·ª´u t∆∞·ª£ng.
            for table_node in parsed_expression.find_all(exp.Table):
                final_table_name = table_node.name
                # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p t√™n b·∫£ng c√≥ alias.
                if hasattr(table_node, 'this') and table_node.this and isinstance(table_node.this, exp.Identifier):
                     final_table_name = table_node.this.name
                if final_table_name:
                    tables.add(final_table_name.lower()) # Chu·∫©n h√≥a v·ªÅ ch·ªØ th∆∞·ªùng.
    except (errors.ParseError, Exception):
        # B·ªè qua n·∫øu sqlglot kh√¥ng th·ªÉ ph√¢n t√≠ch c√∫ ph√°p c√¢u l·ªánh ƒë·ªÉ ch∆∞∆°ng tr√¨nh kh√¥ng b·ªã d·ª´ng.
        pass
        
    return list(tables)

def extract_query_features(sql_query):
    """
    Tr√≠ch xu·∫•t m·ªôt t·∫≠p h·ª£p c√°c ƒë·∫∑c tr∆∞ng s·ªë h·ªçc t·ª´ m·ªôt c√¢u l·ªánh SQL.
    S·ª≠ d·ª•ng sqlglot ƒë·ªÉ ph√¢n t√≠ch c√∫ ph√°p m·ªôt c√°ch hi·ªáu qu·∫£ v√† an to√†n.
    Tr·∫£ v·ªÅ m·ªôt dictionary c√°c ƒë·∫∑c tr∆∞ng s·ªë h·ªçc.
    """
    # Kh·ªüi t·∫°o gi√° tr·ªã m·∫∑c ƒë·ªãnh cho t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng
    features = {
        'num_joins': 0,
        'num_where_conditions': 0,
        'num_group_by_cols': 0,
        'num_order_by_cols': 0,
        'has_limit': 0,
        'has_subquery': 0,
        'has_union': 0,
        'has_where': 0
    }
    
    # Tr·∫£ v·ªÅ m·∫∑c ƒë·ªãnh n·∫øu query kh√¥ng h·ª£p l·ªá
    if pd.isna(sql_query) or not isinstance(sql_query, str) or not sql_query.strip():
        return features

    try:
        parsed = sqlglot.parse_one(sql_query, read='mysql')
        if parsed:
            # === K·ªπ thu·∫≠t t·ªëi ∆∞u t·ª´ code c·ªßa b·∫°n b·∫°n ===
            # ƒê·∫øm s·ªë l∆∞·ª£ng JOIN
            features['num_joins'] = sum(1 for _ in parsed.find_all(exp.Join))
            # Ki·ªÉm tra s·ª± t·ªìn t·∫°i (hi·ªáu qu·∫£ h∆°n .find is not None)
            features['has_limit'] = 1 if parsed.find(exp.Limit) else 0
            features['has_subquery'] = 1 if parsed.find(exp.Subquery) else 0
            features['has_union'] = 1 if parsed.find(exp.Union) else 0
            
            # === Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng chi ti·∫øt h∆°n ===
            # ƒê·∫øm s·ªë l∆∞·ª£ng ƒëi·ªÅu ki·ªán trong WHERE
            where_clause = parsed.find(exp.Where)
            if where_clause:
                features['has_where'] = 1
                # ∆Ø·ªõc t√≠nh s·ªë ƒëi·ªÅu ki·ªán b·∫±ng c√°ch ƒë·∫øm c√°c to√°n t·ª≠ logic
                # (AND, OR) v√† c·ªông 1. find_all nhanh h∆°n walk() cho m·ª•c ƒë√≠ch n√†y.
                # conditions = len(where_clause.find_all(exp.And, exp.Or))
                # ƒê·∫øm s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ m√† kh√¥ng c·∫ßn t·∫°o list
                conditions = sum(1 for _ in where_clause.find_all(exp.And, exp.Or))
                features['num_where_conditions'] = conditions + 1
            
            # ƒê·∫øm s·ªë c·ªôt trong GROUP BY
            group_by_clause = parsed.find(exp.Group)
            if group_by_clause:
                # `group_by_clause.expressions` l√† danh s√°ch c√°c c·ªôt
                features['num_group_by_cols'] = len(group_by_clause.expressions)

            # ƒê·∫øm s·ªë c·ªôt trong ORDER BY
            order_by_clause = parsed.find(exp.Order)
            if order_by_clause:
                features['num_order_by_cols'] = len(order_by_clause.expressions)
            
    except errors.ParseError:
        # N·∫øu sqlglot kh√¥ng parse ƒë∆∞·ª£c, gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh
        pass
    except Exception:
        # B·∫Øt c√°c l·ªói kh√¥ng l∆∞·ªùng tr∆∞·ªõc kh√°c
        pass
        
    return features

def save_feedback_to_csv(item_data: dict, label: int) -> tuple[bool, str]:
    try:
        # item_data ƒë√£ l√† dict:
        identifier_string = f"{item_data.get('timestamp')}{item_data.get('user')}{item_data.get('query')}"
        feedback_id = hashlib.md5(str(identifier_string).encode()).hexdigest()

        new_feedback_data = dict(item_data)  # clone dict
        new_feedback_data['feedback_id'] = feedback_id
        new_feedback_data['is_anomaly_label'] = label

        for k, v in list(new_feedback_data.items()):
            if isinstance(v, (list, tuple)):
                new_feedback_data[k] = json.dumps(v, ensure_ascii=False)

        # ƒë·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
        os.makedirs(os.path.dirname(FEEDBACK_FILE_PATH) or ".", exist_ok=True)

        df_feedback = pd.DataFrame()
        if os.path.exists(FEEDBACK_FILE_PATH) and os.path.getsize(FEEDBACK_FILE_PATH) > 0:
            df_feedback = pd.read_csv(FEEDBACK_FILE_PATH)

        message = ""
        if (not df_feedback.empty and
            'feedback_id' in df_feedback.columns and
            feedback_id in df_feedback['feedback_id'].values):
            message = f"ƒê√£ C·∫¨P NH·∫¨T ph·∫£n h·ªìi cho m·ª•c #{feedback_id[:8]}..."
            idx = df_feedback.index[df_feedback['feedback_id'] == feedback_id][0]
            for col, value in new_feedback_data.items():
                if col in df_feedback.columns:
                    df_feedback.loc[idx, col] = value
                else:
                    df_feedback[col] = pd.NA
                    df_feedback.loc[idx, col] = value
        else:
            message = f"ƒê√£ GHI NH·∫¨N ph·∫£n h·ªìi m·ªõi cho m·ª•c #{feedback_id[:8]}..."
            new_row_df = pd.DataFrame([new_feedback_data])
            df_feedback = pd.concat([df_feedback, new_row_df], ignore_index=True)

        ordered_cols = ['feedback_id', 'timestamp', 'user', 'query', 'is_anomaly_label']
        all_columns = sorted(list(set(df_feedback.columns.tolist() + list(new_feedback_data.keys()))))
        final_cols = [c for c in ordered_cols if c in all_columns] + [c for c in all_columns if c not in ordered_cols]

        df_feedback.to_csv(FEEDBACK_FILE_PATH, mode='w', header=True, index=False, columns=final_cols, encoding='utf-8')
        logging.info(message)
        return True, message
    except Exception as e:
        logging.error(f"ƒê√£ x·∫£y ra l·ªói khi l∆∞u ph·∫£n h·ªìi: {e}")
        import traceback; traceback.print_exc()
        return False, f"ƒê√£ x·∫£y ra l·ªói khi l∆∞u ph·∫£n h·ªìi: {e}"



def save_logs_to_parquet(records: list, source_dbms: str) -> int:
    if not records:
        return 0
    try:
        df = pd.DataFrame(records)
        if 'source_dbms' not in df.columns:
            df['source_dbms'] = source_dbms
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed', utc=True)

        os.makedirs(STAGING_DATA_DIR, exist_ok=True)  # <-- th√™m d√≤ng n√†y

        filename = f"{source_dbms}_{datetime.now().strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex[:8]}.parquet"
        file_path = os.path.join(STAGING_DATA_DIR, filename)
        df.to_parquet(file_path, engine='pyarrow', index=False)
        # logging.info(f"ƒê√£ l∆∞u {len(df)} b·∫£n ghi t·ª´ '{source_dbms}' v√†o file: {filename}")
        return len(df)
    except Exception as e:
        logging.error(f"L·ªói khi l∆∞u file Parquet: {e}")
        return 0

def get_normalized_query(query: str) -> str:
    """Extract DIGEST_TEXT-like normalized query"""
    if not query:
        return ""
    # Simple normalization (you can use sqlglot for better)
    query = re.sub(r'"\w+"', '"?"', query)
    query = re.sub(r"'\w+'", "'?'", query)
    query = re.sub(r'\d+', '?', query)
    return query.strip()

def count_sensitive_tables(tables: list) -> int:
    if not tables:
        return 0
    return len([t for t in tables if any(st in t.lower() for st in SENSITIVE_TABLES)])

def is_late_night(ts):
    from datetime import datetime
    if isinstance(ts, str):
        ts = datetime.fromisoformat(ts.replace("Z", "+00:00"))
    return ts.hour <= 5 or ts.hour >= 23


# ============================================================
#   ACTIVE RESPONSE AUDIT LOGGER
# ============================================================

# C·∫•u h√¨nh logger
audit_logger = logging.getLogger('ActiveResponseAudit')
audit_logger.setLevel(logging.INFO)
audit_logger.propagate = False

# Ch·ªâ th√™m handler n·∫øu n√≥ ch∆∞a c√≥ ƒë·ªÉ tr√°nh log l·∫∑p l·∫°i
if not audit_logger.hasHandlers():
    try:
        # S·ª≠ d·ª•ng 'a' ƒë·ªÉ ghi n·ªëi ti·∫øp, 'utf-8'
        file_handler = logging.FileHandler(ACTIVE_RESPONSE_AUDIT_LOG_PATH, mode='a', encoding='utf-8')
        # ƒê·ªãnh d·∫°ng log: th·ªùi gian v√† n·ªôi dung
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
        audit_logger.addHandler(file_handler)
    except Exception as e:
        print(f"L·ªñI: Kh√¥ng th·ªÉ t·∫°o file audit log t·∫°i {ACTIVE_RESPONSE_AUDIT_LOG_PATH}: {e}")

def log_active_response_action(action: str, target: str, reason: str):
    """
    Ghi l·∫°i m·ªôt h√†nh ƒë·ªông ph·∫£n ·ª©ng ch·ªß ƒë·ªông v√†o file audit log.

    Args:
        action (str): Lo·∫°i h√†nh ƒë·ªông (v√≠ d·ª•: "LOCK_ACCOUNT", "KILL_SESSION").
        target (str): ƒê·ªëi t∆∞·ª£ng b·ªã t√°c ƒë·ªông (v√≠ d·ª•: "user@host", "Session 123").
        reason (str): L√Ω do th·ª±c hi·ªán.
    """
    try:
        message = f"ACTION: {action} | TARGET: {target} | REASON: {reason}"
        audit_logger.info(message)
        for handler in audit_logger.handlers:
            handler.flush()
    except Exception as e:
        print(f"[Active Response] L·ªói khi ghi audit log: {e}")

def generate_html_alert(violation_summary: list):
    """
    T·∫°o n·ªôi dung HTML cho email c·∫£nh b√°o.
    Args:
        violation_summary: List c√°c dict [{'title': 'Gi·ªù Khuya', 'count': 4, 'time': '...', 'desc': '...'}]
    """

    # CSS Inline
    html_template = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <style>
            body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; }}
            .container {{ max_width: 600px; margin: 0 auto; padding: 20px; background-color: #f9f9f9; }}
            .header {{ background-color: #d32f2f; color: white; padding: 20px; text-align: center; border-radius: 5px 5px 0 0; }}
            .alert-box {{ background-color: #fff; padding: 20px; border: 1px solid #ddd; border-top: none; border-radius: 0 0 5px 5px; }}
            .stat-box {{ background-color: #fff3cd; border-left: 5px solid #ffc107; padding: 15px; margin-bottom: 20px; }}
            table {{ width: 100%; border-collapse: collapse; margin-top: 20px; }}
            th {{ background-color: #f2f2f2; text-align: left; padding: 10px; border-bottom: 2px solid #ddd; font-size: 12px; text-transform: uppercase; color: #555; }}
            td {{ padding: 12px 10px; border-bottom: 1px solid #eee; font-size: 14px; }}
            .severity-high {{ color: #d32f2f; font-weight: bold; }}
            .footer {{ text-align: center; font-size: 12px; color: #777; margin-top: 20px; }}
            .btn {{ display: inline-block; background-color: #d32f2f; color: white; padding: 10px 20px; text-decoration: none; border-radius: 3px; margin-top: 20px; font-weight: bold; }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h2 style="margin:0;">üö® Security Alert Triggered</h2>
                <p style="margin:5px 0 0 0; font-size: 14px;">UEBA Detection System</p>
            </div>

            <div class="alert-box">
                <p>The UEBA system has detected abnormal behaviors that require your attention.</p>

                <div class="stat-box">
                    <strong>Overview:</strong> Detected <strong>{len(violation_summary)}</strong> type/s of anomalies in the latest scan.
                </div>

                <table width="100%">
                    <thead>
                        <tr>
                            <th>Type of Violation</th>
                            <th style="text-align: center;">Count</th>
                            <th>Entity (User@IP)</th>
                            <th>Occurrence (First - Last)</th>
                        </tr>
                    </thead>
                    <tbody>
    """

    # Loop ƒë·ªÉ t·∫°o c√°c d√≤ng trong b·∫£ng
    for item in violation_summary:
        html_template += f"""
            <tr>
                <td>
                    <span class="severity-high">{item['title']}</span><br>
                    <span style="font-size: 11px; color: #777;">{item['desc']}</span>
                </td>
                <td style="text-align: center;"><strong>{item['count']}</strong></td>
                <td style="font-size: 13px; color: #333;">
                    {item.get('target_str', 'N/A')}
                </td>
                <td style="font-size: 13px; white-space: nowrap;">
                    {item.get('time_range', 'N/A')}
                </td>
            </tr>
        """
    html_template += """
                    </tbody>
                </table>

            </div>

            <div class="footer" style="
                margin-top: 25px;
                text-align: center;
                background: #f5f5f5;
                padding: 15px 10px;
                border-top: 2px solid #d0d0d0;
                font-size: 12px;
                font-style: italic;
                color: #555;
            ">
                <p>
                    This is an automated email from the UEBA Platform system.<br>
                    Please do not reply to this email.
                </p>
            </div>

        </div>
    </body>
    </html>
    """
    return html_template

# ==============================================================================
#   C√ÅC H√ÄM H·ªñ TR·ª¢ PH√ÇN T√çCH THEO LU·∫¨T (RULE-BASED ANALYSIS)
# ==============================================================================

# ==============================================================================
# 1. NH√ìM ACCESS ANOMALIES (B·∫•t th∆∞·ªùng truy c·∫≠p)
# Bao g·ªìm: Concurrent Login, Brute-force, Impossible Travel
# ==============================================================================
def check_access_anomalies(df, rule_config):
    """Nh√≥m 1: B·∫•t th∆∞·ªùng truy c·∫≠p"""
    anomalies = {}
    thresholds = rule_config.get('thresholds', {})
    settings = rule_config.get('settings', {})
    
    # Rule 1. Concurrent Login
    limit_ips = thresholds.get('concurrent_ips_limit', 1)
    df_sorted = df.sort_values('timestamp')
    grouped = df_sorted.groupby(['user', pd.Grouper(key='timestamp', freq='5Min')], observed=False)
    idx_concurrent = []
    for (user, time), group in grouped:
        if group['client_ip'].nunique() > limit_ips:
            idx_concurrent.extend(group.index.tolist()) 
    if idx_concurrent:
        anomalies['Concurrent Login'] = list(set(idx_concurrent))
                
    # Rule 2. Brute-force Success
    limit_attempts = thresholds.get('brute_force_attempts', 5)
    failed_attempts = df[df['error_code'] != 0] # Error code != 0 l√† l·ªói
    idx_bruteforce = [] 
    if not failed_attempts.empty:
        ip_error_counts = failed_attempts.groupby(['client_ip', pd.Grouper(key='timestamp', freq='1Min')], observed=False).size()
        suspicious_ips = ip_error_counts[ip_error_counts > limit_attempts].index.get_level_values(0).unique()     
        # T√¨m log th√†nh c√¥ng ngay sau chu·ªói l·ªói t·ª´ IP ƒë√≥
        brute_force_success = df[
            (df['client_ip'].isin(suspicious_ips)) & 
            (df['error_code'] == 0) & 
            (df['event_name'] == 'connect')
        ]
        idx_bruteforce.extend(brute_force_success.index.tolist())      
    if idx_bruteforce:
        anomalies['Brute-force Success'] = list(set(idx_bruteforce))
        
    # Rule 3. Impossible Travel
    if HAS_GEOIP:
        max_speed = thresholds.get('impossible_travel_speed_kmh', 800)
        db_path = settings.get('geoip_db_path', 'engine/geoip/GeoLite2-City.mmdb')
        idx_travel = []  
        if os.path.exists(db_path):        
            try:
                reader = geoip2.database.Reader(db_path)
                def get_lat_lon(ip):
                    try:
                        if ip in ['127.0.0.1', 'localhost', '::1', '0.0.0.0']: return None
                        res = reader.city(ip)
                        return (res.location.latitude, res.location.longitude)
                    except: return None              
                # Group by User ƒë·ªÉ check h√†nh tr√¨nh
                grouped_user = df_sorted.groupby('user', observed=False)
                for user, group in grouped_user:
                    if len(group) < 2: continue
                    prev_row = None
                    for idx, row in group.iterrows():
                        curr_loc = get_lat_lon(row['client_ip'])
                        if prev_row and curr_loc and prev_row['loc']:
                            dist = geodesic(prev_row['loc'], curr_loc).km
                            time_diff = (row['timestamp'] - prev_row['time']).total_seconds() / 3600
                            if time_diff > 0 and dist > 50:
                                speed = dist / time_diff
                                if speed > max_speed:
                                    idx_travel.append(idx)
                        if curr_loc:
                            prev_row = {'loc': curr_loc, 'time': row['timestamp']}
                reader.close()
            except Exception as e:
                logging.error(f"GeoIP Logic Error: {e}")             
        if idx_travel:
            anomalies['Impossible Travel'] = list(set(idx_travel))

    return anomalies

# ============================================================================================================================================================
# 2. NH√ìM INSIDER THREATS (M·ªëi ƒëe d·ªça n·ªôi b·ªô)
# Bao g·ªìm: Service Account, Admin Privilege Escalation, Sensitive Access, Late Night, Ghost Account, System Table Modification, Insecure Connection
# ============================================================================================================================================================
def check_insider_threats(df, rule_config):
    """Nh√≥m 2: Insider Threat"""
    anomalies = {}
    service_accounts = rule_config.get('service_accounts', {})
    signatures = rule_config.get('signatures', {})
    settings = rule_config.get('settings', {})
    hr_users = set(signatures.get('hr_authorized_users', []))
    
    # Rule 4. Service Account Misuse
    idx_service = []
    overtime_schedule = signatures.get('overtime_schedule', [])
    for user, config in service_accounts.items():
        user_logs = df[df['user'] == user]
        if user_logs.empty: continue
        # 1. Check IP (Gi·ªØ nguy√™n logic c≈© - IP sai l√† b·∫Øt lu√¥n, kh√¥ng c√≥ ngo·∫°i l·ªá overtime cho IP)
        invalid_ip = user_logs[~user_logs['client_ip'].isin(config.get('allowed_ips', []))]
        idx_service.extend(invalid_ip.index.tolist())
        # 2. Check Gi·ªù (Logic m·ªõi: Sai gi·ªù chu·∫©n -> Check ti·∫øp v√© Overtime)
        # L·∫•y ra nh·ªØng d√≤ng log vi ph·∫°m gi·ªù chu·∫©n
        potential_hour_violations = user_logs[~user_logs['timestamp'].dt.hour.isin(config.get('allowed_hours', []))]
        if potential_hour_violations.empty:
            continue
        # Duy·ªát qua t·ª´ng log vi ph·∫°m gi·ªù ƒë·ªÉ xem c√≥ "v√©" overtime kh√¥ng
        for idx, row in potential_hour_violations.iterrows():
            is_excused = False
            # N·∫øu c√≥ danh s√°ch overtime, ƒëi ki·ªÉm tra
            if overtime_schedule:
                current_time = row['timestamp'].time()
                current_date_str = row['timestamp'].strftime('%Y-%m-%d')
                for shift in overtime_schedule:
                    # So kh·ªõp User v√† Ng√†y
                    if shift.get('user') == user and shift.get('date') == current_date_str:
                        try:
                            allowed_start = dt_time.fromisoformat(shift['start'])
                            allowed_end = dt_time.fromisoformat(shift['end'])
                            
                            # Ki·ªÉm tra gi·ªù log c√≥ n·∫±m trong khung gi·ªù xin ph√©p kh√¥ng
                            if allowed_start <= allowed_end:
                                if allowed_start <= current_time <= allowed_end:
                                    is_excused = True
                                    break
                            else: # X·ª≠ l√Ω qua ƒë√™m (v√≠ d·ª• 23h ƒë·∫øn 2h s√°ng)
                                if allowed_start <= current_time or current_time <= allowed_end:
                                    is_excused = True
                                    break
                        except:
                            continue
            # N·∫øu KH√îNG ƒë∆∞·ª£c tha (kh√¥ng kh·ªõp l·ªãch overtime n√†o) -> Th√™m v√†o danh s√°ch l·ªói
            if not is_excused:
                idx_service.append(idx)    
    if idx_service: anomalies['Service Account Misuse'] = list(set(idx_service))

    # Rule 5. Admin Privilege Escalation
    idx_admin = []
    admin_kws = signatures.get('admin_keywords', [])
    if admin_kws:
        pattern = "|".join(re.escape(k) for k in admin_kws)
        admin_actions = df[
            (df['query'].str.contains(pattern, case=False, na=False)) &
            (df['user'] != 'root')
        ]
        idx_admin.extend(admin_actions.index.tolist())       
    if idx_admin: anomalies['Admin Privilege Abuse'] = list(set(idx_admin))
        
    # Rule 6. Sensitive Table Access 
    idx_sensitive = []
    sensitive_tables = signatures.get('sensitive_tables', [])
    allowed_users = settings.get('sensitive_allowed_users', [])   
    safe_start = settings.get('sensitive_safe_hours_start', 8)
    safe_end = settings.get('sensitive_safe_hours_end', 17) 
    # H√†m check logic
    def is_violation(row):
        # 1. Ki·ªÉm tra xem query c√≥ ƒë·ª•ng v√†o b·∫£ng nh·∫°y c·∫£m kh√¥ng?
        is_sensitive_query = False
        for tbl in sensitive_tables:
            if tbl in str(row['query']):
                is_sensitive_query = True
                break
        if not is_sensitive_query:
            return False # Kh√¥ng vi ph·∫°m v√¨ kh√¥ng ƒë·ª•ng b·∫£ng nh·∫°y c·∫£m
        # 2. Ki·ªÉm tra User
        if row['user'] not in allowed_users:
            return True # Vi ph·∫°m nghi√™m tr·ªçng: User kh√¥ng c√≥ quy·ªÅn m√† truy c·∫≠p
        # 3. Ki·ªÉm tra Th·ªùi gian (D√†nh cho User ƒê√É C√ì QUY·ªÄN)
        # Ngay c·∫£ khi c√≥ quy·ªÅn, n·∫øu truy c·∫≠p ngo√†i gi·ªù h√†nh ch√≠nh c≈©ng b·ªã coi l√† b·∫•t th∆∞·ªùng
        try:
            hour = row['timestamp'].hour
            # N·∫øu gi·ªù hi·ªán t·∫°i nh·ªè h∆°n gi·ªù b·∫Øt ƒë·∫ßu HO·∫∂C l·ªõn h∆°n gi·ªù k·∫øt th√∫c
            if hour < safe_start or hour >= safe_end:
                return True # Vi ph·∫°m: User c√≥ quy·ªÅn nh∆∞ng truy c·∫≠p sai gi·ªù (v√≠ d·ª•: k·∫ø to√°n truy c·∫≠p b·∫£ng l∆∞∆°ng l√∫c 3h s√°ng)
        except:
            pass
        return False # H·ª£p l·ªá (ƒê√∫ng ng∆∞·ªùi, ƒë√∫ng gi·ªù)
    if sensitive_tables:
        sensitive_violation = df[df.apply(is_violation, axis=1)]
        idx_sensitive.extend(sensitive_violation.index.tolist())   
    if idx_sensitive: anomalies['Sensitive Table Access'] = list(set(idx_sensitive))

    # Rule 7. Late Night Query 
    # Logic: Truy c·∫≠p ngo√†i gi·ªù h√†nh ch√≠nh (22h - 5h s√°ng)
    idx_latenight = []
    try:
        # L·∫•y c·∫•u h√¨nh gi·ªù khuya
        s_str = settings.get('late_night_start', '22:00:00')
        e_str = settings.get('late_night_end', '05:00:00')
        start_time_limit = dt_time.fromisoformat(s_str)
        end_time_limit = dt_time.fromisoformat(e_str)
        # L·∫•y danh s√°ch ƒëƒÉng k√Ω l√†m th√™m gi·ªù/b·∫£o tr√¨
        overtime_schedule = signatures.get('overtime_schedule', [])
        def is_late_night_violation(row):
            ts = row['timestamp']
            current_time = ts.time()
            current_date_str = ts.strftime('%Y-%m-%d')
            user_name = row['user']
            current_ip = row['client_ip'] # L·∫•y IP hi·ªán t·∫°i
            # 1. Ki·ªÉm tra khung gi·ªù khuya
            is_night = False
            if start_time_limit <= end_time_limit:
                is_night = start_time_limit <= current_time <= end_time_limit
            else:
                is_night = start_time_limit <= current_time or current_time <= end_time_limit           
            if not is_night:
                return False # Kh√¥ng ph·∫£i gi·ªù khuya -> Kh√¥ng vi ph·∫°m
            # 2. Check Overtime Schedule (V√© th√¥ng h√†nh)
            if overtime_schedule:
                for shift in overtime_schedule:
                    # Check User v√† Date
                    if shift.get('user') == user_name and shift.get('date') == current_date_str:
                        # N·∫øu trong l·ªãch c√≥ ƒëƒÉng k√Ω IP, th√¨ b·∫Øt bu·ªôc ph·∫£i kh·ªõp
                        registered_ip = shift.get('ip')
                        if registered_ip and registered_ip.strip():
                            # N·∫øu IP ƒëƒÉng k√Ω kh√°c IP ƒëang truy c·∫≠p -> Kh√¥ng ch·∫•p nh·∫≠n v√© n√†y
                            if current_ip != registered_ip.strip():
                                continue 
                        try:
                            allowed_start = dt_time.fromisoformat(shift['start'])
                            allowed_end = dt_time.fromisoformat(shift['end'])                            
                            is_authorized = False
                            if allowed_start <= allowed_end:
                                is_authorized = allowed_start <= current_time <= allowed_end
                            else:
                                is_authorized = allowed_start <= current_time or current_time <= allowed_end                            
                            if is_authorized:
                                return False # H·ª£p l·ªá (ƒê√∫ng ng∆∞·ªùi, ƒë√∫ng gi·ªù, ƒë√∫ng IP)
                        except ValueError:
                            continue
            # 3. Kh√¥ng c√≥ v√© ho·∫∑c v√© kh√¥ng kh·ªõp -> Vi ph·∫°m
            return True
        late_night_logs = df[df.apply(is_late_night_violation, axis=1)]
        idx_latenight.extend(late_night_logs.index.tolist())
    except Exception as e:
        logging.error(f"Rule 7 Logic Error: {e}")   
    if idx_latenight: anomalies['Late Night Query'] = list(set(idx_latenight))
    
    # Rule 8. Ghost Account Creation
    idx_ghost = []
    create_cmds = df[df['query'].str.contains("CREATE USER", case=False, na=False)]
    if not create_cmds.empty:
        pattern = re.compile(r"CREATE\s+USER\s+['\"`]?([a-zA-Z0-9_]+)['\"`]?", re.IGNORECASE)
        for idx, row in create_cmds.iterrows():
            match = pattern.search(row['query'])
            if match and match.group(1) not in hr_users:
                idx_ghost.append(idx)               
    if idx_ghost: anomalies['Ghost Account Creation'] = list(set(idx_ghost))

    # Rule 9. System Table Modification
    # Logic: Can thi·ªáp b·∫£ng h·ªá th·ªëng nh∆∞ng kh√¥ng ph·∫£i l·ªánh SELECT/SHOW
    idx_sys_mod = []
    if 'is_system_table' in df.columns:
        # L·ªçc c√°c d√≤ng t√°c ƒë·ªông b·∫£ng h·ªá th·ªëng
        sys_access = df[df['is_system_table'] == 1]
        if not sys_access.empty:
            # Lo·∫°i tr·ª´ c√°c l·ªánh ch·ªâ ƒë·ªçc (SELECT, SHOW, DESCRIBE)
            # L∆∞u √Ω: event_name th∆∞·ªùng l√† 'statement/sql/update', 'statement/sql/insert'...
            # C√°ch ƒë∆°n gi·∫£n: query kh√¥ng b·∫Øt ƒë·∫ßu b·∫±ng SELECT/SHOW
            sys_mod = sys_access[~sys_access['query'].str.match(r'^\s*(SELECT|SHOW|DESC)', case=False, na=False)]
            idx_sys_mod.extend(sys_mod.index.tolist())
    if idx_sys_mod: anomalies['System Table Modification'] = list(set(idx_sys_mod))

    # Rule 10. Insecure Connection
    # Logic: C√°c user quan tr·ªçng (nh∆∞ root) k·∫øt n·ªëi qua TCP/IP (th∆∞·ªùng kh√¥ng an to√†n n·∫øu kh√¥ng c√≥ SSL) thay v√¨ Socket
    idx_insecure = []
    restricted_users = signatures.get('restricted_connection_users', ['root', 'admin'])
    if 'connection_type' in df.columns:
        insecure_conns = df[
            (df['user'].isin(restricted_users)) &
            (df['connection_type'] == 'TCP/IP')  # Socket th∆∞·ªùng l√† 'Localhost via UNIX socket'
        ]
        idx_insecure.extend(insecure_conns.index.tolist())
    if idx_insecure: anomalies['Insecure Connection'] = list(set(idx_insecure))
    
    return anomalies

# ============================================================================================================================================================
# 3. NH√ìM TECHNICAL ATTACKS (T·∫•n c√¥ng k·ªπ thu·∫≠t)
# Bao g·ªìm: SQLi, DoS, High CPU Usage, Scan Efficiency, Config Change, Entropy, Client Mismatch, Disk Temp Table Abuse,
#          Excessive Locking, Suspicious Comment, Warning Flooding, Password Hash Attack, JSON Data Extraction
# ============================================================================================================================================================
def check_technical_attacks(df, rule_config):
    """Nh√≥m 3: Technical Attacks"""
    anomalies = {}
    thresholds = rule_config.get('thresholds', {})
    signatures = rule_config.get('signatures', {})
    
    # Rule 11. SQL Injection
    idx_sqli = []
    sqli_kws = signatures.get('sqli_keywords', [])
    if sqli_kws:
        pattern_sqli = "|".join(re.escape(k) for k in sqli_kws)
        sqli_logs = df[df['query'].str.contains(pattern_sqli, case=False, na=False)]
        idx_sqli.extend(sqli_logs.index.tolist())
    if idx_sqli: anomalies['SQL Injection'] = list(set(idx_sqli))
    
    # Rule 12. DoS / Resource Exhaustion
    idx_dos = []
    max_time = thresholds.get('execution_time_limit_ms', 5000)
    idx_dos.extend(df[df['execution_time_ms'] > max_time].index.tolist())
    if idx_dos: anomalies['DoS / Resource Exhaustion'] = list(set(idx_dos))
    
    # Rule 13. High CPU Usage 
    idx_cpu = []
    max_cpu_time = thresholds.get('cpu_time_limit_ms', 1000) # 1s CPU
    if 'cpu_time_ms' in df.columns:
        high_cpu = df[df['cpu_time_ms'] > max_cpu_time]
        idx_cpu.extend(high_cpu.index.tolist())
    if idx_cpu: anomalies['High CPU Usage'] = list(set(idx_cpu))
    
    # Rule 14. Scan Efficiency
    idx_scan = []
    min_eff = thresholds.get('scan_efficiency_min', 0.01)
    min_rows = thresholds.get('scan_efficiency_min_rows', 1000)
    inefficient = df[
        (df['rows_examined'] > min_rows) & 
        (df['rows_returned'] < (df['rows_examined'] * min_eff))
    ]
    idx_scan.extend(inefficient.index.tolist())
    if idx_scan: anomalies['Scan Efficiency'] = list(set(idx_scan))
    
    # Rule 15. Config Change
    idx_config = []
    config_change = df[
        df['query'].str.contains("SET GLOBAL|general_log", regex=True, case=False, na=False)
    ]
    idx_config.extend(config_change.index.tolist())
    if idx_config: anomalies['Config Change'] = list(set(idx_config))
    
    # Rule 16. High Entropy Queries
    idx_entropy = []
    max_entropy = thresholds.get('max_query_entropy', 4.8)
    if 'query_entropy' in df.columns:
        high_entropy = df[df['query_entropy'] > max_entropy]
        idx_entropy.extend(high_entropy.index.tolist())
    if idx_entropy: anomalies['High Entropy Query'] = list(set(idx_entropy))

    # Rule 17. Client/OS Mismatch
    # Ph√°t hi·ªán tool t·∫•n c√¥ng trong blacklist (sqlmap, nmap...)
    idx_client = []
    whitelist = signatures.get('allowed_programs', [])
    if 'program_name' in df.columns and not whitelist:
        pattern_bad = "|".join(re.escape(p) for p in whitelist)
        bad_clients = df[df['program_name'].str.contains(pattern_bad, case=False, na=False)]
        idx_client.extend(bad_clients.index.tolist())
    if idx_client: anomalies['Client Mismatch'] = list(set(idx_client))

    # Rule 18. Disk Temp Table Abuse
    # Logic: Query t·∫°o b·∫£ng t·∫°m tr√™n ·ªï c·ª©ng (g√¢y ch·∫≠m)
    idx_disk_abuse = []
    if 'created_tmp_disk_tables' in df.columns:
        disk_abusers = df[df['created_tmp_disk_tables'] > 0]
        idx_disk_abuse.extend(disk_abusers.index.tolist())
    if idx_disk_abuse: anomalies['Disk Temp Table Abuse'] = list(set(idx_disk_abuse))

    # Rule 19. Index Evasion / Full Join
    # Logic: Kh√¥ng d√πng index ho·∫∑c Full Join
    idx_no_index = []
    if 'no_index_used' in df.columns and 'select_full_join' in df.columns:
        bad_scans = df[(df['no_index_used'] == 1) | (df['select_full_join'] == 1)]
        # Ch·ªâ b·∫Øt n·∫øu qu√©t nhi·ªÅu d√≤ng ƒë·ªÉ tr√°nh false positive cho b·∫£ng nh·ªè
        bad_scans = bad_scans[bad_scans['rows_examined'] > 100] 
        idx_no_index.extend(bad_scans.index.tolist())
    if idx_no_index: anomalies['Index Evasion'] = list(set(idx_no_index))

    # Rule 20. Excessive Locking
    # Logic: Lock qu√° l√¢u
    idx_locking = []
    lock_limit = thresholds.get('lock_time_limit_ms', 500) # 0.5s
    if 'lock_time_ms' in df.columns:
        long_locks = df[df['lock_time_ms'] > lock_limit]
        idx_locking.extend(long_locks.index.tolist())
    if idx_locking: anomalies['Excessive Locking'] = list(set(idx_locking))

    # Rule 21. Suspicious Comment
    # Logic: C√≥ comment nh∆∞ng kh√¥ng ph·∫£i job h·ªá th·ªëng
    idx_comments = []
    if 'has_comment' in df.columns and 'is_system_table' in df.columns:
        suspicious_cmts = df[
            (df['has_comment'] == 1) & 
            (df['is_system_table'] == 0)
        ]
        idx_comments.extend(suspicious_cmts.index.tolist())
    if idx_comments: anomalies['Suspicious Comment'] = list(set(idx_comments))

    # Rule 22. Warning Flooding
    # Logic: Query sinh ra qu√° nhi·ªÅu warning
    idx_warnings = []
    warn_thresh = thresholds.get('warning_count_threshold', 5)
    if 'warning_count' in df.columns:
        warn_floods = df[df['warning_count'] > warn_thresh]
        idx_warnings.extend(warn_floods.index.tolist())
    if idx_warnings: anomalies['Warning Flooding'] = list(set(idx_warnings))

    # Rule 23. Password Hash Attack
    # Logic: C·ªë t√¨nh select chu·ªói x√°c th·ª±c
    idx_pass_attack = []
    pass_attack = df[df['query'].str.contains(r'authentication_string|password_expired', regex=True, case=False, na=False)]
    idx_pass_attack.extend(pass_attack.index.tolist())
    if idx_pass_attack: anomalies['Password Hash Attack'] = list(set(idx_pass_attack))

    # Rule 24. JSON Data Extraction
    # Logic: D√πng h√†m JSON extract
    idx_json = []
    json_extract = df[df['query'].str.contains(r'JSON_EXTRACT|JSON_UNQUOTE|->>', regex=True, case=False, na=False)]
    idx_json.extend(json_extract.index.tolist())
    if idx_json: anomalies['JSON Data Extraction'] = list(set(idx_json))

    return anomalies

# ============================================================================================================================================================
# 4. NH√ìM DATA DESTRUCTION (Ph√° ho·∫°i d·ªØ li·ªáu)
# Bao g·ªìm: Mass Delete, Old Data, Large Dump, Audit Log Manipulation, Hidden View / Rename
# ============================================================================================================================================================
def check_data_destruction(df, rule_config):
    """Nh√≥m 4: Data Destruction"""
    anomalies = {}
    thresholds = rule_config.get('thresholds', {})
    signatures = rule_config.get('signatures', {})
    
    # Rule 25. Mass Deletion (X√≥a h√†ng lo·∫°t)
    idx_delete = []
    limit_rows = thresholds.get('mass_deletion_rows', 500)
    mass_delete = df[
        (df['event_name'].isin(['statement/sql/delete', 'statement/sql/drop'])) &
        (df['rows_affected'] > limit_rows)
    ]
    idx_delete.extend(mass_delete.index.tolist())
    if idx_delete: anomalies['Mass Deletion'] = list(set(idx_delete))
    
    # Rule 26. Old Data Modification (S·ª≠a d·ªØ li·ªáu c≈©)
    idx_old_data = []
    old_data_access = df[df['query'].str.contains("2019|2020|2021|2022|2023|2024", regex=True, na=False)]
    idx_old_data.extend(old_data_access.index.tolist())
    if idx_old_data: anomalies['Old Data Modification'] = list(set(idx_old_data))

    # Rule 27. Large Data Dump 
    # Logic: Select b·∫£ng quan tr·ªçng m√† tr·∫£ v·ªÅ qu√° nhi·ªÅu d√≤ng
    idx_dump = []
    large_dump_tables = signatures.get('large_dump_tables', [])
    if large_dump_tables:
        pattern_dump = "|".join(re.escape(k) for k in large_dump_tables)
        # ƒêi·ªÅu ki·ªán: Query ch·ª©a t√™n b·∫£ng quan tr·ªçng V√Ä tr·∫£ v·ªÅ > 1000 d√≤ng
        dump_logs = df[
            (df['query'].str.contains(pattern_dump, case=False, na=False)) &
            (df['rows_returned'] > 1000)
        ]
        idx_dump.extend(dump_logs.index.tolist())
    if idx_dump: anomalies['Large Data Dump'] = list(set(idx_dump))

    # Rule 28. Audit Log Manipulation
    # Logic: Update/Delete tr√™n b·∫£ng log/audit/history
    idx_audit = []
    # Regex t√¨m t√™n b·∫£ng ch·ª©a ch·ªØ log, audit, history
    audit_tables_pattern = r'(?:general_log|audit_log|slow_log|history)'
    audit_manipulation = df[
        (df['query'].str.contains(audit_tables_pattern, regex=True, case=False, na=False)) &
        (df['event_name'].isin(['statement/sql/delete', 'statement/sql/update', 'statement/sql/truncate']))
    ]
    idx_audit.extend(audit_manipulation.index.tolist())
    if idx_audit: anomalies['Audit Log Manipulation'] = list(set(idx_audit))

    # Rule 29. Hidden View / Rename
    # Logic: ƒê·ªïi t√™n b·∫£ng ho·∫∑c t·∫°o View
    idx_obfuscation = []
    # T√¨m l·ªánh RENAME TABLE ho·∫∑c CREATE VIEW
    obfuscation_cmds = df[df['query'].str.contains(r'RENAME\s+TABLE|CREATE\s+VIEW', regex=True, case=False, na=False)]
    idx_obfuscation.extend(obfuscation_cmds.index.tolist())
    if idx_obfuscation: anomalies['Hidden View / Rename'] = list(set(idx_obfuscation))
    
    return anomalies

# ==============================================================================
# 5. RULE 30: MULTI-TABLE ACCESS 
# ==============================================================================
def check_multi_table_anomalies(df, rule_config):
    """
    Rule 30: Multi-table Access
    """
    anomalies = []
    thresholds = rule_config.get('thresholds', {})
    
    # L·∫•y tham s·ªë
    window_min = thresholds.get('multi_table_window_minutes', 4)
    min_tables = thresholds.get('multi_table_min_count', 3)
    
    def extract_tables_list(q):
        if not isinstance(q, str) or not q.strip(): return []
        
        # 1. ∆Øu ti√™n SQLGlot
        if SQLGLOT_AVAILABLE:
            try:
                tables = get_tables_with_sqlglot(q)
                if tables: return tables
            except: pass

        # 2. Fallback Regex
        try:
            pattern = r'(?:\bFROM\b|\bJOIN\b|\bUPDATE\b|\bINTO\b)\s+(?!\()([`\'"]?\w+[`\'"]?(?:\.[`\'"]?\w+[`\'"]?)?)'
            matches = re.findall(pattern, q, re.IGNORECASE)
            clean_tables = []
            for m in matches:
                clean_name = m.replace('`', '').replace("'", "").replace('"', "").lower()
                clean_tables.append(clean_name)
            return clean_tables
        except:
            return []

    target_events = ['SELECT', 'SHOW', 'DESCRIBE']
    mask = df['query'].str.contains('|'.join(target_events), case=False, na=False)
    df_target = df[mask].copy()
    
    if df_target.empty: return {} # <-- S·ª¨A: Tr·∫£ v·ªÅ dict r·ªóng thay v√¨ list r·ªóng

    df_target['accessed_tables_list'] = df_target['query'].apply(extract_tables_list)
    df_target = df_target[df_target['accessed_tables_list'].map(len) > 0]
    df_target = df_target.sort_values('timestamp')
    
    grouped = df_target.groupby(['user', pd.Grouper(key='timestamp', freq=f'{window_min}Min')], observed=False)

    for (user, time_window), group in grouped:
        unique_tables_in_window = set()
        for tbl_list in group['accessed_tables_list']:
            unique_tables_in_window.update(tbl_list)
            
        if len(unique_tables_in_window) > min_tables:
            anomalies.extend(group.index.tolist())

    # --- [QUAN TR·ªåNG] S·ª¨A ƒêO·∫†N RETURN T·∫†I ƒê√ÇY ---
    if anomalies:
        # Tr·∫£ v·ªÅ Dictionary: { 'T√™n Rule': [Danh s√°ch Index] }
        return {'Multi-Table Access': list(set(anomalies))}
    
    return {}

# ==============================================================================
# 5. RULE 31: BEHAVIORAL PROFILE
# ==============================================================================
def update_behavior_redis(redis_client, df_logs):
    """
    H·ªçc th√≥i quen: C·∫≠p nh·∫≠t t·∫ßn su·∫•t ho·∫°t ƒë·ªông c·ªßa User theo gi·ªù v√†o Redis.
    S·ª≠ d·ª•ng Pipeline ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô ghi.
    """
    if df_logs.empty or redis_client is None:
        return

    try:
        pipe = redis_client.pipeline()
        
        # Ch·ªâ quan t√¢m c√°c c·ªôt c·∫ßn thi·∫øt
        for _, row in df_logs.iterrows():
            user = row.get('user')
            timestamp = row.get('timestamp')
            
            if user and timestamp:
                # Key: uba:profile:thanh.nguyen
                key = f"{REDIS_PROFILE_KEY_PREFIX}{user}"
                
                # Field: Gi·ªù (0-23)
                hour = str(timestamp.hour)
                
                # TƒÉng bi·∫øn ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán t·∫°i gi·ªù n√†y th√™m 1
                pipe.hincrby(key, hour, 1)
                
                # Gia h·∫°n th·ªùi gian s·ªëng cho Key (n·∫øu c·∫ßn)
                if PROFILE_TTL_SECONDS:
                    pipe.expire(key, PROFILE_TTL_SECONDS)
        
        # Th·ª±c thi h√†ng lo·∫°t l·ªánh
        pipe.execute()
        logging.info(f"Updated behavior profiles for {len(df_logs)} logs.")
        
    except Exception as e:
        logging.error(f"Error updating Redis profile: {e}")
        
def check_behavior_redis(redis_client, df_logs, min_threshold=5):
    """
    Ki·ªÉm tra b·∫•t th∆∞·ªùng: So s√°nh log hi·ªán t·∫°i v·ªõi l·ªãch s·ª≠ trong Redis.
    min_threshold: S·ªë l·∫ßn xu·∫•t hi·ªán t·ªëi thi·ªÉu ƒë·ªÉ coi l√† b√¨nh th∆∞·ªùng.
    """
    anomalies_indices = []
    
    if df_logs.empty or redis_client is None:
        return anomalies_indices

    try:
        for idx, row in df_logs.iterrows():
            user = row.get('user')
            timestamp = row.get('timestamp')
            
            if user and timestamp:
                key = f"{REDIS_PROFILE_KEY_PREFIX}{user}"
                hour = str(timestamp.hour)
                
                # L·∫•y s·ªë l·∫ßn ƒë√£ xu·∫•t hi·ªán
                count_bytes = redis_client.hget(key, hour)
                
                count = 0
                if count_bytes:
                    try:
                        count = int(count_bytes)
                    except:
                        count = 0
                
                # LOGIC PH√ÅT HI·ªÜN: D√πng tham s·ªë min_threshold truy·ªÅn v√†o
                if count < min_threshold:
                    anomalies_indices.append(idx)

    except Exception as e:
        logging.error(f"Error checking Redis profile: {e}")
        return []

    return anomalies_indices

# ==============================================================================
# REDIS CONFIGURATION HELPER
# ==============================================================================

def configure_redis_for_reliability(redis_client: Redis) -> bool:
    """
    Configure Redis for better reliability and handle MISCONF errors.
    
    Returns:
        bool: True if configuration was successful, False otherwise
    """
    try:
        # Strategy 1: Use AOF instead of RDB for better persistence
        try:
            redis_client.config_set("save", "")  # Disable RDB snapshots
            logging.info("‚úÖ Redis: Disabled RDB snapshots")
            
            redis_client.config_set("appendonly", "yes")  # Enable AOF
            logging.info("‚úÖ Redis: Enabled AOF persistence")
            
            return True
            
        except Exception as config_error:
            logging.warning(f"‚ö†Ô∏è Could not configure Redis persistence: {config_error}")
            
            # Strategy 2: Fallback - disable the error check
            try:
                redis_client.config_set("stop-writes-on-bgsave-error", "no")
                logging.warning("‚ö†Ô∏è Redis: Disabled RDB error checking (fallback)")
                return True
                
            except Exception as fallback_error:
                logging.warning(f"‚ö†Ô∏è Redis: Could not modify config: {fallback_error}")
                return False
                
    except Exception as e:
        logging.error(f"‚ùå Redis configuration failed: {e}")
        return False

def handle_redis_misconf_error(error_msg: str) -> str:
    """
    Provide helpful error message and suggestions for MISCONF errors.
    
    Args:
        error_msg: The Redis error message
        
    Returns:
        str: Helpful suggestion message
    """
    if "MISCONF" in error_msg:
        return (
            "üí° Redis MISCONF Error Solutions:\n"
            "   1. Check disk space: df -h\n"
            "   2. Check Redis logs: tail -f /var/log/redis/redis-server.log\n"
            "   3. Fix permissions: sudo chown redis:redis /var/lib/redis\n"
            "   4. Disable RDB: redis-cli CONFIG SET save ''\n"
            "   5. Enable AOF: redis-cli CONFIG SET appendonly yes"
        )
    return "Check Redis server status and logs"